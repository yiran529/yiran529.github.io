<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 8.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/27/linux_cmd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/27/linux_cmd/" class="post-title-link" itemprop="url">Linux 进阶常用命令实战：grep 与 sed 详解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-12-27 10:00:00" itemprop="dateCreated datePublished" datetime="2025-12-27T10:00:00+08:00">2025-12-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-18 11:19:41" itemprop="dateModified" datetime="2025-12-18T11:19:41+08:00">2025-12-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">技术笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          除了 cd 和 rm，掌握 grep 和 sed 才是 Linux 高效运维的关键。本文总结了日志分析与文本处理的常见实战场景。
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/12/27/linux_cmd/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/26/CrossValidation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/26/CrossValidation/" class="post-title-link" itemprop="url">深入解析交叉验证：从原理到面试实战</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-26 10:00:00 / Modified: 13:32:14" itemprop="dateCreated datePublished" datetime="2025-12-26T10:00:00+08:00">2025-12-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这是一个非常棒的面试准备方向。<strong>交叉验证（Cross-Validation, CV）</strong> 是评估模型泛化能力的核心手段，也是机器学习面试中考察“数据敏感度”和“工程实践能力”的高频考点。</p>
<p>我将为你撰写一篇进阶版的技术文章，不仅讲透 <strong>K折交叉验证</strong>，还会延伸至 <strong>超参数调优</strong>、<strong>过拟合检测</strong> 以及 <strong>面试中常见的“坑”</strong>。这篇文章同样符合Hexo规范，你可以作为“面试突击”系列的第二篇发布。</p>
<hr>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 面试硬核知识：吃透交叉验证（K-Fold）与模型评估体系</span><br><span class="line">date: 2025-12-21 10:00:00</span><br><span class="line">tags: [Machine Learning, Cross Validation, K-Fold, Interview Prep, Model Evaluation]</span><br><span class="line">categories: [面试宝典, 机器学习]</span><br><span class="line"><span class="section">mathjax: true</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"></span><br><span class="line">在机器学习面试中，面试官问完算法原理后，紧接着通常会问：“你是如何评估模型效果的？”如果你只回答“看准确率”，那你可能就危险了。</span><br><span class="line"></span><br><span class="line">本文将深入解析<span class="strong">**交叉验证（Cross-Validation）**</span>，特别是<span class="strong">**K折验证**</span>的底层逻辑，并以此为原点，辐射出面试中必问的<span class="strong">**模型评估、偏差方差权衡**</span>及<span class="strong">**超参数调优**</span>等核心知识点。</span><br><span class="line"></span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line"></span><br><span class="line"><span class="section">## 1. 为什么要用交叉验证？</span></span><br><span class="line"><span class="section">### ——打破“单次测试”的偶然性</span></span><br><span class="line"></span><br><span class="line">在最简单的机器学习流程中，我们将数据分为 <span class="strong">**训练集 (Train Set)**</span> 和 <span class="strong">**测试集 (Test Set)**</span>。但这存在两个致命问题：</span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**数据浪费**</span>：测试集的数据被“隔离”了，模型从未见过它们，如果数据量本就很少，这很奢侈。</span><br><span class="line"><span class="bullet">2.</span>  <span class="strong">**运气成分（方差高）**</span>：如果切分时运气不好，测试集里全是很难分类的样本，模型得分会很低；反之则虚高。<span class="strong">**一次划分的结果是不可靠的。**</span></span><br><span class="line"></span><br><span class="line"><span class="strong">**交叉验证的核心思想**</span>：让所有数据都有机会既当“训练员”，又当“考官”，从而得出更稳健的模型评分。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="section">## 2. 核心主角：K折交叉验证 (K-Fold CV)</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 2.1 算法流程</span></span><br><span class="line">K折交叉验证是最常用的CV方法，通常 K 取 5 或 10。</span><br><span class="line"></span><br><span class="line"><span class="strong">**步骤拆解**</span>：</span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**分割**</span>：将全部训练数据 $D$ 随机打乱，均匀分成 $K$ 份（Subsets/Folds），记为 $\&#123;F<span class="emphasis">_1, F_</span>2, ..., F<span class="emphasis">_K\&#125;$。</span></span><br><span class="line"><span class="emphasis">2.  <span class="strong">**循环**</span>：进行 $K$ 轮训练与验证：</span></span><br><span class="line"><span class="emphasis">    *   第 1 轮：取 $F_</span>1$ 做验证集，其余 $\&#123;F<span class="emphasis">_2, ..., F_</span>K\&#125;$ 做训练集 -&gt; 得到分数 $S<span class="emphasis">_1$。</span></span><br><span class="line"><span class="emphasis">    *   第 2 轮：取 $F_</span>2$ 做验证集，其余 $\&#123;F<span class="emphasis">_1, F_</span>3, ..., F<span class="emphasis">_K\&#125;$ 做训练集 -&gt; 得到分数 $S_</span>2$。</span><br><span class="line"><span class="bullet">    *</span>   ...</span><br><span class="line"><span class="bullet">    *</span>   第 K 轮：取 $F<span class="emphasis">_K$ 做验证集，其余做训练集 -&gt; 得到分数 $S_</span>K$。</span><br><span class="line"><span class="bullet">3.</span>  <span class="strong">**平均**</span>：最终的模型得分是 $K$ 次分数的平均值：$\bar&#123;S&#125; = \frac&#123;1&#125;&#123;K&#125;\sum<span class="emphasis">_&#123;i=1&#125;^K S_</span>i$。</span><br><span class="line"></span><br><span class="line"><span class="section">### 2.2 K-Fold 的优势</span></span><br><span class="line"><span class="bullet">*</span>   <span class="strong">**更稳健**</span>：减少了因数据划分不同导致的评分波动。</span><br><span class="line"><span class="bullet">*</span>   <span class="strong">**全数据利用**</span>：每一个样本都被用作过验证集（1次）和训练集（K-1次）。</span><br><span class="line"></span><br><span class="line"><span class="section">### 2.3 代码实现 (Python/Scikit-learn)</span></span><br><span class="line"><span class="code">```python</span></span><br><span class="line"><span class="code">from sklearn.model_selection import cross_val_score, KFold</span></span><br><span class="line"><span class="code">from sklearn.ensemble import RandomForestClassifier</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code"># 假设 X, y 是你的数据</span></span><br><span class="line"><span class="code">rf = RandomForestClassifier()</span></span><br><span class="line"><span class="code"># 定义5折</span></span><br><span class="line"><span class="code">kf = KFold(n_splits=5, shuffle=True, random_state=42)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code"># 计算5次得分</span></span><br><span class="line"><span class="code">scores = cross_val_score(rf, X, y, cv=kf, scoring=&#x27;accuracy&#x27;)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">print(f&quot;每折得分: &#123;scores&#125;&quot;)</span></span><br><span class="line"><span class="code">print(f&quot;平均得分: &#123;scores.mean():.4f&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="3-面试延伸：你必须知道的变体与“坑”"><a href="#3-面试延伸：你必须知道的变体与“坑”" class="headerlink" title="3. 面试延伸：你必须知道的变体与“坑”"></a>3. 面试延伸：你必须知道的变体与“坑”</h2><p>面试官常会给出特定场景，考察你对CV变体的选择。</p>
<h3 id="3-1-分层-K-折-Stratified-K-Fold"><a href="#3-1-分层-K-折-Stratified-K-Fold" class="headerlink" title="3.1 分层 K 折 (Stratified K-Fold)"></a>3.1 分层 K 折 (Stratified K-Fold)</h3><ul>
<li><strong>场景</strong>：<strong>类别不平衡 (Imbalanced Data)</strong>。例如：癌症预测数据中，99%是健康，1%是患病。</li>
<li><strong>问题</strong>：普通的随机K折可能导致某一折全是健康样本，模型无法在验证集中学到患病特征。</li>
<li><strong>解决</strong>：Stratified K-Fold 保证每一折中，各类别的<strong>比例</strong>与原始数据集保持一致。</li>
<li><strong>面试金句</strong>：<em>“在处理分类任务，尤其是类别不平衡时，我通常优先使用 Stratified K-Fold 而非普通 K-Fold。”</em></li>
</ul>
<h3 id="3-2-留一法-Leave-One-Out-LOOCV"><a href="#3-2-留一法-Leave-One-Out-LOOCV" class="headerlink" title="3.2 留一法 (Leave-One-Out, LOOCV)"></a>3.2 留一法 (Leave-One-Out, LOOCV)</h3><ul>
<li><strong>场景</strong>：<strong>数据量极少</strong>（例如只有50个样本）。</li>
<li><strong>逻辑</strong>：$K = N$（样本总数）。每次只留1个样本做验证，其余 $N-1$ 个做训练。</li>
<li><strong>缺点</strong>：计算量巨大，且方差较高（因为训练集之间几乎完全相同）。</li>
</ul>
<h3 id="3-3-时间序列分割-Time-Series-Split-——-面试高频坑"><a href="#3-3-时间序列分割-Time-Series-Split-——-面试高频坑" class="headerlink" title="3.3 时间序列分割 (Time Series Split) —— 面试高频坑"></a>3.3 时间序列分割 (Time Series Split) —— <strong>面试高频坑</strong></h3><ul>
<li><strong>场景</strong>：股票预测、销量预测等<strong>时序数据</strong>。</li>
<li><strong>禁忌</strong>：<strong>绝对不能用普通的 K-Fold！</strong></li>
<li><strong>原因</strong>：普通 K-Fold 会随机打乱数据，导致“用未来的数据训练，去预测过去的数据”（数据泄露，Data Leakage），效果会虚高。</li>
<li><strong>解决</strong>：使用 <code>TimeSeriesSplit</code>。第1次用前100天预测第101天；第2次用前101天预测第102天… 始终保持<strong>训练集时间 &lt; 验证集时间</strong>。</li>
</ul>
<hr>
<h2 id="4-知识点串联：从CV看机器学习全局"><a href="#4-知识点串联：从CV看机器学习全局" class="headerlink" title="4. 知识点串联：从CV看机器学习全局"></a>4. 知识点串联：从CV看机器学习全局</h2><p>准备面试时，不要孤立地看CV，要把它和以下三个核心概念串联起来：</p>
<h3 id="4-1-超参数调优-Hyperparameter-Tuning"><a href="#4-1-超参数调优-Hyperparameter-Tuning" class="headerlink" title="4.1 超参数调优 (Hyperparameter Tuning)"></a>4.1 超参数调优 (Hyperparameter Tuning)</h3><p>交叉验证最实用的场景是<strong>网格搜索 (Grid Search)</strong>。</p>
<ul>
<li><strong>问题</strong>：如何确定随机森林的树的数量（n_estimators）是100还是200？</li>
<li><strong>方法</strong>：不能在测试集上试（会作弊），必须在训练集内部做CV。</li>
<li><strong>流程</strong>：设定参数组合 -&gt; 对每组参数做 K-Fold -&gt; 取平均分最高的参数 -&gt; <strong>用该参数在全部训练集上重新训练模型</strong> -&gt; 最终在测试集上评估。</li>
</ul>
<h3 id="4-2-偏差与方差-Bias-vs-Variance"><a href="#4-2-偏差与方差-Bias-vs-Variance" class="headerlink" title="4.2 偏差与方差 (Bias vs Variance)"></a>4.2 偏差与方差 (Bias vs Variance)</h3><p>CV 可以帮助我们判断模型状态：</p>
<ul>
<li><strong>高偏差 (Underfitting)</strong>：训练集CV分数低，验证集CV分数也低。说明模型太简单（如线性模型拟合复杂数据）。</li>
<li><strong>高方差 (Overfitting)</strong>：训练集CV分数极高，但验证集CV分数低，且<strong>各折之间的分数波动很大</strong>。说明模型死记硬背。</li>
</ul>
<h3 id="4-3-评估指标-Metrics"><a href="#4-3-评估指标-Metrics" class="headerlink" title="4.3 评估指标 (Metrics)"></a>4.3 评估指标 (Metrics)</h3><p>做CV时，<code>scoring</code> 参数选什么很重要。面试官常问：“如果是欺诈检测，你还看准确率吗？”</p>
<ul>
<li><strong>准确率 (Accuracy)</strong>：类别平衡时用。</li>
<li><strong>精确率 (Precision) &amp; 召回率 (Recall)</strong>：类别不平衡时用。<ul>
<li><em>查准 (Precision)</em>：预测为正的样本里，多少是真的？（宁缺毋滥）</li>
<li><em>查全 (Recall)</em>：所有真的正样本里，找出了多少？（宁可错杀一千，不可放过一个）</li>
</ul>
</li>
<li><strong>F1-Score</strong>：Precision和Recall的调和平均。</li>
<li><strong>AUC-ROC</strong>：衡量模型对正负样本的排序能力，<strong>对类别不平衡不敏感</strong>，非常常用。</li>
</ul>
<hr>
<h2 id="5-面试实战问答-Cheatsheet"><a href="#5-面试实战问答-Cheatsheet" class="headerlink" title="5. 面试实战问答 (Cheatsheet)"></a>5. 面试实战问答 (Cheatsheet)</h2><p><strong>Q1: K折验证的 K 选多少合适？</strong></p>
<blockquote>
<p><strong>A:</strong> 通常选 5 或 10。</p>
<ul>
<li>$K$ 值越大（如 LOOCV）：偏差(Bias)越小（训练集接近原始大小），但计算成本高，且方差(Variance)可能变大。</li>
<li>$K$ 值越小（如 3）：计算快，但偏差可能较大。</li>
<li>5或10是偏差、方差和算力的最佳平衡点。</li>
</ul>
</blockquote>
<p><strong>Q2: 训练完交叉验证后，最终的模型是什么？</strong></p>
<blockquote>
<p><strong>A:</strong> 这是一个陷阱题。交叉验证的目的<strong>只是为了评估参数或模型的效果</strong>，而不是为了得到最终模型。</p>
<ul>
<li>CV 过程中产生了 $K$ 个临时模型，评估完后通常会丢弃。</li>
<li>确定好最优超参数后，我们需要使用<strong>全部训练数据 (All Training Data)</strong> 重新训练出一个最终模型，用来去测试集或生产环境预测。</li>
</ul>
</blockquote>
<p><strong>Q3: 数据预处理（如归一化）应该在CV之前做还是CV内部做？</strong></p>
<blockquote>
<p><strong>A:</strong> <strong>一定要在 CV 循环内部做！</strong>（这是为了防止数据泄露）。<br>如果你在分割前就对所有数据做了归一化，那么验证集的信息（均值、方差）就泄露到了训练集中。正确做法是构建 Pipeline，在每一折训练时，只根据当折的训练集计算均值方差，然后应用到验证集。</p>
</blockquote>
<hr>
<p>希望这篇总结能帮你在面试中从容应对关于模型评估的连环追问！加油！<br>```</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/26/GPUCPU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/26/GPUCPU/" class="post-title-link" itemprop="url">GPU 架构与深度学习调度全解析（含面试高频题）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-26 10:00:00 / Modified: 13:32:20" itemprop="dateCreated datePublished" datetime="2025-12-26T10:00:00+08:00">2025-12-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>GPU 架构与深度学习调度全解析（含面试高频题）<br>一、GPU 架构基础（面向深度学习）</p>
<ol>
<li>CPU vs GPU 的设计哲学对比<br>维度<br>CPU<br>GPU<br>设计目标<br>低延迟、复杂控制<br>高吞吐、大规模并行<br>核心数量<br>少（4–64 个）<br>多（几千个 CUDA cores）<br>控制逻辑<br>复杂（分支预测、乱序执行）<br>极简<br>适合任务<br>串行、控制密集型<br>数据并行（矩阵、张量运算）</li>
</ol>
<p>深度学习的本质是大规模数值计算（矩阵乘、卷积），天然适合 GPU 并行加速。</p>
<ol>
<li>GPU 整体硬件结构（NVIDIA CUDA 架构）<br>2.1 逻辑层次结构<br>GPU<br>├── GPC (Graphics Processing Cluster)<br>│   └── SM (Streaming Multiprocessor)<br>│       ├── CUDA Cores（标量/向量运算）<br>│       ├── Tensor Cores（矩阵乘专用，支持 FP16/BF16/INT8）<br>│       ├── Register File（寄存器）<br>│       ├── Shared Memory / L1 Cache（片上高速存储）<br>│       └── Warp Scheduler（线程束调度器）<br>└── Global Memory (HBM / GDDR，主显存)</li>
</ol>
<p>2.2 核心执行单元：SM（Streaming Multiprocessor）<br>SM 是 GPU 的核心执行单元，所有线程最终映射到 SM 执行，核心特性：<br>包含 CUDA Cores、Tensor Cores 等计算资源，以及寄存器、Shared Memory 等存储资源；<br>单个 SM 理论最大驻留 warp 数：Kepler 架构 48 个，Maxwell 及以上（Ampere、Hopper）64 个；<br>实际驻留 warp 数受寄存器、共享内存占用限制，通常低于理论值；<br>调度的最小硬件单位在 SM 内部（以 warp 为单位）。</p>
<ol>
<li>CUDA 执行模型（核心）<br>3.1 Kernel / Grid / Block / Thread 层级结构<br>Grid（Kernel 启动单位）<br>├── Block 0（线程块，最小独立调度单位）<br>│   ├── Thread 0<br>│   ├── Thread 1<br>│   └── …<br>├── Block 1<br>└── …</li>
</ol>
<p>层级<br>含义<br>核心特点<br>Thread<br>单个执行流<br>最小执行实体<br>Block<br>线程块<br>内部线程可共享 Shared Memory<br>Grid<br>Kernel 启动的整体单位<br>不同 Block 彼此独立，无共享资源</p>
<p>3.2 Warp（硬件调度核心）<br>定义：1 个 warp = 32 个线程（NVIDIA GPU 固定值，调度最小单位）；<br>执行模式：SIMT（Single Instruction, Multiple Threads），同一 warp 内线程执行相同指令；<br>关键问题：分支会导致 warp divergence（线程束分化），使串行化执行，降低性能。</p>
<ol>
<li>GPU 内存层次结构<br>4.1 内存金字塔（速度从快到慢，容量从小到大）<br>Registers → Shared Memory / L1 Cache → L2 Cache → Global Memory</li>
</ol>
<p>内存类型<br>访问速度<br>核心作用<br>Registers<br>★★★★★<br>存储线程局部变量<br>Shared Memory<br>★★★★☆<br>Block 内线程共享数据<br>L2 Cache<br>★★★☆☆<br>跨 SM 共享的缓存<br>Global Memory<br>★☆☆☆☆<br>主显存，存储大规模数据</p>
<p>深度学习优化核心：减少 Global Memory 访问（速度最慢，延迟最高），尽量利用寄存器和 Shared Memory。<br>二、GPU 调度机制（硬件层）</p>
<ol>
<li>Warp 调度（延迟隐藏核心）<br>GPU 不依赖复杂控制逻辑隐藏延迟，而是通过 快速切换 warp 实现：<br>当 Warp A 等待内存访问时，Warp Scheduler 立即切换到就绪的 Warp B/C 执行；<br>核心目标：保持 GPU 计算单元始终繁忙，掩盖访存延迟。</li>
<li>Occupancy（占用率）<br>定义：当前 SM 中活跃 warp 数 / SM 理论最大 warp 数；<br>影响因素：每线程寄存器占用量、每 Block 共享内存占用量、Block 大小（threads per block）；<br>关键结论：高 Occupancy ≠ 高性能，但过低的 Occupancy 必然导致性能瓶颈（无法有效隐藏延迟）。<br>三、深度学习中的 GPU 调度（软件层，PyTorch 实战）</li>
<li>CUDA Stream（异步调度核心）<br>1.1 核心概念<br>Stream 是 GPU 上的任务队列；<br>同一 Stream：任务顺序执行；<br>不同 Stream：任务可并行执行（充分利用 GPU 资源）。<br>1.2 PyTorch 示例<br>import torch</li>
</ol>
<p>s1 = torch.cuda.Stream()<br>s2 = torch.cuda.Stream()</p>
<h1 id="两个-Stream-并行执行"><a href="#两个-Stream-并行执行" class="headerlink" title="两个 Stream 并行执行"></a>两个 Stream 并行执行</h1><p>with torch.cuda.stream(s1):<br>    a = torch.randn(1024, 1024, device=’cuda’)  # Kernel 1</p>
<p>with torch.cuda.stream(s2):<br>    b = torch.randn(1024, 1024, device=’cuda’)  # Kernel 2（可能与 Kernel 1 并行）</p>
<ol>
<li>同步（Synchronization）命令<br>2.1 全局同步<br>torch.cuda.synchronize()  # 等待所有 Stream 执行完成，常用于计时</li>
</ol>
<p>2.2 Stream 级同步<br>s1.synchronize()  # 仅等待 Stream s1 中所有任务完成</p>
<p>2.3 事件（Event）：精确计时<br>start = torch.cuda.Event(enable_timing=True)<br>end = torch.cuda.Event(enable_timing=True)</p>
<p>start.record()  # 记录开始事件</p>
<h1 id="执行需要计时的-GPU-操作（如模型前向传播）"><a href="#执行需要计时的-GPU-操作（如模型前向传播）" class="headerlink" title="执行需要计时的 GPU 操作（如模型前向传播）"></a>执行需要计时的 GPU 操作（如模型前向传播）</h1><p>y = model(x)<br>end.record()    # 记录结束事件</p>
<p>torch.cuda.synchronize()  # 确保 GPU 操作完成<br>print(f”执行时间：{start.elapsed_time(end):.2f} ms”)  # 输出毫秒数</p>
<ol>
<li>Kernel Launch 调度（PyTorch 层特性）<br>默认异步执行：Python 代码返回不代表 GPU 计算完成（如 y = x @ x 后立即 print(“done”) 可能先打印）；<br>隐式同步触发条件：调用 .item()、.cpu()、torch.cuda.synchronize() 时，CPU 会阻塞等待 GPU 完成。</li>
<li>多 GPU 调度<br>4.1 基础：指定 GPU<h1 id="运行时指定使用-GPU-0-和-1"><a href="#运行时指定使用-GPU-0-和-1" class="headerlink" title="运行时指定使用 GPU 0 和 1"></a>运行时指定使用 GPU 0 和 1</h1>CUDA_VISIBLE_DEVICES=0,1 python train.py</li>
</ol>
<p>4.2 PyTorch 代码中设备选择<br>device = torch.device(“cuda:0”)  # 指定 GPU 0<br>model.to(device)  # 模型移到 GPU<br>data = data.to(device)  # 数据移到 GPU</p>
<p>4.3 分布式调度（DDP）</p>
<h1 id="单机器-8-GPU-训练（推荐使用-torchrun）"><a href="#单机器-8-GPU-训练（推荐使用-torchrun）" class="headerlink" title="单机器 8 GPU 训练（推荐使用 torchrun）"></a>单机器 8 GPU 训练（推荐使用 torchrun）</h1><p>torchrun —nproc_per_node=8 train.py</p>
<p>底层依赖：NCCL 通信库、Ring-AllReduce 梯度聚合、Stream 实现计算与通信重叠。</p>
<ol>
<li>高级调度技术<br>5.1 计算与通信重叠（Overlap）<br>核心思路：将梯度计算（GPU 计算）和 AllReduce 通信（多 GPU 数据传输）放入不同 Stream；<br>优势：减少通信等待时间，提升整体训练吞吐量（DDP 已默认支持）。<br>5.2 CUDA Graph（减少调度开销）<br>g = torch.cuda.CUDAGraph()<br>with torch.cuda.graph(g):<br> y = model(x)  # 录制静态计算流程</li>
</ol>
<p>适用场景：shape 固定的任务（如 Transformer/DiT/MoE 推理）；<br>优势：减少 Kernel Launch 开销，提升小批量任务性能。<br>5.3 MoE 特有调度优化<br>MoE（混合专家模型）的核心调度挑战：<br>问题：Token 到 Expert 的路由、Expert 负载不均、All-to-All 通信开销；<br>优化方向：Expert 并行调度、Top-k 路由、Token Batching、Stream-aware Expert Execution。<br>四、核心总结（考试 / 报告友好）<br>GPU 通过 Warp 级调度 + 大规模并行 + 异步执行 实现高吞吐，深度学习框架（PyTorch/TensorFlow）通过 CUDA Stream、Event、DDP、CUDA Graph 等机制，在软件层最大化硬件利用率，平衡计算与通信效率。<br>五、面试高频问答（可直接背诵）<br>一、GPU 架构类<br>Q1：GPU 和 CPU 的核心区别是什么？为什么深度学习更适合 GPU？<br>标准回答：CPU 以低延迟、复杂控制为目标，核心少但功能强；GPU 以高吞吐为目标，拥有大量简单计算核心，适合大规模数据并行任务。深度学习的矩阵乘、卷积是高度数据并行计算，因此更适合 GPU。<br>加分点：GPU 通过 SIMT + Warp 调度隐藏访存延迟，且简化控制逻辑，将硬件面积更多分配给算力单元。<br>Q2：什么是 SM？SM 在 GPU 中起什么作用？<br>标准回答：SM（Streaming Multiprocessor）是 GPU 的基本执行单元，负责执行 CUDA Kernel，包含 CUDA Cores、Tensor Cores、寄存器、Shared Memory 等资源，所有线程最终映射到 SM 执行。<br>加分点：Kernel 的 Block 只能在一个 SM 内执行，SM 之间完全独立，无共享资源。<br>Q3：什么是 Warp？为什么是 32 个线程？<br>标准回答：Warp 是 GPU 的最小调度单位，由 32 个线程组成，GPU 以 Warp 为单位发射指令，同一 Warp 内线程执行相同指令。32 是硬件设计的权衡结果，兼顾 SIMD 宽度与调度复杂度。<br>加分点：Warp Divergence 会导致串行执行，降低效率；一个 Block 由多个 Warp 组成（需是 32 的整数倍）。<br>Q4：什么是 Warp Divergence？如何避免？<br>标准回答：Warp Divergence 指同一 Warp 内线程因分支条件不同执行不同路径，导致串行化执行。避免方式：减少条件分支、重排数据使同 Warp 线程执行相同逻辑、使用 Mask 计算替代分支。<br>加分点：Divergence 仅影响单个 Warp 内线程，不影响其他 Warp；if-else 中不同路径都会被执行，仅未命中分支的线程闲置。<br>Q5：GPU 的内存层次结构是怎样的？<br>标准回答：GPU 内存从快到慢、容量从小到大依次为：Registers → Shared Memory/L1 Cache → L2 Cache → Global Memory。优化核心是减少 Global Memory 访问（延迟最高）。<br>加分点：Shared Memory 由程序员手动管理，灵活性高；寄存器溢出（线程占用寄存器过多）会导致数据写入 Global Memory，显著降速。<br>二、GPU 调度与执行模型类<br>Q6：GPU 如何隐藏访存延迟？<br>标准回答：GPU 不依赖乱序执行，而是通过在同一 SM 内同时驻留多个 Warp，当某个 Warp 等待内存时，调度器切换到其他就绪 Warp 执行，从而掩盖延迟。<br>加分点：足够的 Occupancy 是延迟隐藏的前提，否则调度器无可用 Warp 切换。<br>Q7：什么是 Occupancy？高 Occupancy 一定好吗？<br>标准回答：Occupancy 是 SM 中活跃 Warp 数与理论最大 Warp 数的比值，影响延迟隐藏能力，但高 Occupancy 不一定等于高性能 —— 计算密集型任务即使低 Occupancy 也可能表现优异。<br>加分点：Occupancy 受寄存器、Shared Memory 占用量限制；IO 密集型任务需更高 Occupancy 掩盖访存延迟。<br>Q8：Block 为什么不能跨 SM 执行？<br>标准回答：因为 Block 内线程需要共享 Shared Memory 和同步原语（如 __syncthreads()），这些资源仅存在于单个 SM 内，跨 SM 执行会带来巨大通信和同步成本。<br>加分点：Grid 级别无原生同步原语，需通过 Stream 或 Event 实现跨 Block 同步。<br>三、深度学习框架（PyTorch）类<br>Q9：PyTorch 的 GPU 运算是同步还是异步的？<br>标准回答：默认是异步的。Python 代码返回不代表 GPU 计算完成，仅当调用 .item()、.cpu()、torch.cuda.synchronize() 等操作时，CPU 才会阻塞等待 GPU 完成。<br>加分点：异步执行可实现 CPU-GPU 并行（CPU 准备数据，GPU 同时计算）；计时时必须显式同步，否则结果不准确。<br>Q10：什么是 CUDA Stream？有什么作用？<br>标准回答：CUDA Stream 是 GPU 上的任务队列，同一 Stream 内任务顺序执行，不同 Stream 间可并行执行，核心作用是实现计算与通信重叠、多任务并行，提升 GPU 利用率。<br>加分点：默认 Stream 是 per-device 的；Stream 是 DDP 实现计算 - 通信重叠的基础。<br>Q11：如何实现计算与通信重叠？<br>标准回答：将计算任务（如梯度计算）和通信任务（如 AllReduce）分别放入不同 CUDA Stream，结合异步通信库（如 NCCL），使两类任务在 GPU 上并行执行。<br>加分点：DDP 已默认集成该优化；需注意任务间依赖关系，避免数据竞争。<br>Q12：torch.cuda.synchronize () 的作用？为什么不常用？<br>标准回答：该函数会阻塞 CPU 线程，等待 GPU 上所有 Stream 的任务执行完成。不常用是因为频繁同步会破坏异步执行的性能优势，仅用于调试或精确计时。<br>加分点：stream.synchronize() 是细粒度同步，仅等待指定 Stream，比全局同步更灵活。<br>四、多 GPU / 分布式类<br>Q13：DDP 和 DataParallel 的区别？<br>标准回答：DataParallel 是单进程多线程架构，存在 GIL 锁和主 GPU 通信瓶颈；DDP 是多进程架构，每个 GPU 绑定一个进程，使用 NCCL 通信，效率更高，是大规模训练的推荐方案。<br>加分点：DDP 支持多机器训练，DataParallel 仅支持单机器；DDP 通过 Ring-AllReduce 聚合梯度，通信成本更低。<br>Q14：AllReduce 是什么？为什么重要？<br>标准回答：AllReduce 是分布式训练中用于梯度聚合的核心通信操作，能让所有 GPU 节点获得相同的梯度总和（或平均值），是同步 SGD 的基础。<br>加分点：Ring-AllReduce 是常用实现，通信复杂度为 O (N)，比集中式聚合更高效；通信成本是分布式训练的主要瓶颈之一。<br>五、高阶加分题（MoE/Transformer 相关）<br>Q15：MoE 中最大的 GPU 调度挑战是什么？<br>标准回答：核心挑战是 Expert 负载不均（部分 Expert 处理大量 Token，部分闲置）和 Token 路由导致的 All-to-All 通信开销，容易造成 GPU 利用率低和通信瓶颈。<br>加分点：可通过 Expert 并行、动态负载均衡、Token Batching 等方式优化；Stream-aware 执行能减少 Expert 调度延迟。<br>Q16：CUDA Graph 的作用是什么？适合哪些场景？<br>标准回答：CUDA Graph 将一段 GPU 计算流程静态化，提前录制 Kernel 依赖关系，减少 Runtime 调度和 Kernel Launch 开销。适合 shape 固定的场景（如 Transformer/DiT 推理、静态批量训练）。<br>加分点：对动态控制流（如动态 Batch Size、条件分支）不友好；对小 Kernel 性能提升尤为明显（Launch 开销占比高）。<br>终极总结句（强烈建议背诵）<br>GPU 通过 Warp 级调度和大规模并行隐藏延迟，而深度学习框架通过异步执行、CUDA Stream、通信重叠和分布式调度，在软件层最大化硬件利用率。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/26/diffusion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/26/diffusion/" class="post-title-link" itemprop="url">从 DDPM 到 DDIM 的演进推导</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-26 10:00:00 / Modified: 13:32:18" itemprop="dateCreated datePublished" datetime="2025-12-26T10:00:00+08:00">2025-12-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="从-DDPM-到-DDIM-的演进推导：定义前向-gt-求解分布-gt-寻找反向步-gt-破除马尔可夫约束"><a href="#从-DDPM-到-DDIM-的演进推导：定义前向-gt-求解分布-gt-寻找反向步-gt-破除马尔可夫约束" class="headerlink" title="从 DDPM 到 DDIM 的演进推导：定义前向 -&gt; 求解分布 -&gt; 寻找反向步 -&gt; 破除马尔可夫约束"></a>从 DDPM 到 DDIM 的演进推导：定义前向 -&gt; 求解分布 -&gt; 寻找反向步 -&gt; 破除马尔可夫约束</h1><p>为建立严密逻辑链条，系统总结与推导脉络如下：</p>
<h2 id="一、核心-Motivation：从“物理模拟”到“数学加速”"><a href="#一、核心-Motivation：从“物理模拟”到“数学加速”" class="headerlink" title="一、核心 Motivation：从“物理模拟”到“数学加速”"></a>一、核心 Motivation：从“物理模拟”到“数学加速”</h2><h3 id="（一）DDPM：定义游戏规则"><a href="#（一）DDPM：定义游戏规则" class="headerlink" title="（一）DDPM：定义游戏规则"></a>（一）DDPM：定义游戏规则</h3><ul>
<li><strong>目标</strong>：学习反向马尔可夫链，将纯高斯噪声还原为数据。  </li>
<li><strong>痛点</strong>：采样效率极低。反向推导严格依赖相邻时刻马尔可夫假设（$x_{t - 1}$ 需由 $x_t$ 得到），生成图像需循环 1000 步，无法跳步。  </li>
</ul>
<h3 id="（二）DDIM：打破“相邻”的枷锁"><a href="#（二）DDIM：打破“相邻”的枷锁" class="headerlink" title="（二）DDIM：打破“相邻”的枷锁"></a>（二）DDIM：打破“相邻”的枷锁</h3><ul>
<li><strong>核心改进动机</strong>  <ul>
<li><strong>加速采样</strong>：构建可“跳步”的采样公式（如从 $t = 100$ 直接跳到 $t = 50$  ）。  </li>
<li><strong>确定性映射</strong>：让初值噪声 $x_T$ 与生成图像 $x_0$ 一一对应，支持图像编辑（Inversion）。  </li>
</ul>
</li>
<li><strong>变与不变</strong>  <ul>
<li><strong>不变</strong>：训练目标（Objective）不变。DDIM 发现 DDPM 训练仅依赖边缘分布 $q(x<em>t|x_0)$，可直接复用 DDPM 训练好的 $\epsilon</em>\theta$ 模型。  </li>
<li><strong>变化</strong>：前向过程假设改变。DDPM 假设前向为马尔可夫过程；DDIM 重新定义“非马尔可夫”前向过程，边缘分布与 DDPM 保持一致。  </li>
</ul>
</li>
</ul>
<h2 id="二、DDPM-核心推导：马尔可夫链的闭环"><a href="#二、DDPM-核心推导：马尔可夫链的闭环" class="headerlink" title="二、DDPM 核心推导：马尔可夫链的闭环"></a>二、DDPM 核心推导：马尔可夫链的闭环</h2><p>DDPM 推导分“前向加噪”和“反向去噪”两步。  </p>
<h3 id="（一）Step-1：前向过程（扩散）"><a href="#（一）Step-1：前向过程（扩散）" class="headerlink" title="（一）Step 1：前向过程（扩散）"></a>（一）Step 1：前向过程（扩散）</h3><ul>
<li><strong>单步定义</strong>：给定 $x<em>{t - 1}$，下一步 $x_t$ 增加少量噪声：<br>$q(x_t | x</em>{t - 1}) = \mathcal{N}(x<em>t; \sqrt{1 - \beta_t}x</em>{t - 1}, \beta_t\mathbf{I})$  </li>
<li><strong>任意时刻推导</strong>：利用递归性质 + 重参数化技巧，直接写出 $x_t$ 关于 $x_0$ 的分布（令 $\alpha_t = 1 - \beta_t$）：<br>$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I})$<br>这是扩散模型核心性质：已知原始数据，可直接采样任意时刻噪声图像。  </li>
</ul>
<h3 id="（二）Step-2：反向过程（采样）"><a href="#（二）Step-2：反向过程（采样）" class="headerlink" title="（二）Step 2：反向过程（采样）"></a>（二）Step 2：反向过程（采样）</h3><ul>
<li><strong>核心难点</strong>：无法直接求 $q(x_{t - 1}|x_t)$。  </li>
<li><strong>贝叶斯中转</strong>：利用贝叶斯公式 $q(x<em>{t - 1}|x_t, x_0) = q(x_t|x</em>{t - 1}, x<em>0) \frac{q(x</em>{t - 1}|x_0)}{q(x_t|x_0)}$，右边三项均为已知高斯分布。  </li>
<li><strong>采样公式</strong>：代入高斯分布密度函数计算，求出 $q(x<em>{t - 1}|x_t, x_0)$ 均值。因未知真实 $x_0$，模型 $\epsilon</em>\theta$ 预测 $x<em>t$ 噪声间接预测 $x_0$：<br>$x</em>{t - 1} = \frac{1}{\sqrt{\alpha<em>t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon</em>\theta(x_t, t) \right) + \sigma_t z, \quad z \sim \mathcal{N}(0, \mathbf{I})$  </li>
</ul>
<h2 id="三、DDIM-核心推导：重构前向概率流"><a href="#三、DDIM-核心推导：重构前向概率流" class="headerlink" title="三、DDIM 核心推导：重构前向概率流"></a>三、DDIM 核心推导：重构前向概率流</h2><p>DDIM 关键洞察：只要 $q(x_t|x_0)$ 不变，反向过程无需遵循马尔可夫链。  </p>
<h3 id="（一）Step-1：定义非马尔可夫前向过程"><a href="#（一）Step-1：定义非马尔可夫前向过程" class="headerlink" title="（一）Step 1：定义非马尔可夫前向过程"></a>（一）Step 1：定义非马尔可夫前向过程</h3><p>DDIM 构造含超参数 $\sigma_t$ 的通用分布形式：  </p>
<p>$q<em>\sigma(x</em>{t - 1}|x<em>t, x_0) = \mathcal{N}(x</em>{t - 1}; \text{mean}, \sigma_t^2\mathbf{I})$ </p>
<p>为保证边缘分布 $q(x_t|x_0)$ 与 DDPM 一致，均值项设计为三部分组合。  </p>
<h3 id="（二）Step-2：统一采样公式（核心结论）"><a href="#（二）Step-2：统一采样公式（核心结论）" class="headerlink" title="（二）Step 2：统一采样公式（核心结论）"></a>（二）Step 2：统一采样公式（核心结论）</h3><p>基于上述构造，DDIM 推导出采样公式：<br>$x<em>{t - 1} = \underbrace{\sqrt{\bar{\alpha}</em>{t - 1}} \left( \frac{x<em>t - \sqrt{1 - \bar{\alpha}_t}\epsilon</em>\theta(x<em>t, t)}{\sqrt{\bar{\alpha}_t}} \right)}</em>{\text{预测的 } x<em>0 \text{ 部分}} + \underbrace{\sqrt{1 - \bar{\alpha}</em>{t - 1} - \sigma<em>t^2} \cdot \epsilon</em>\theta(x<em>t, t)}</em>{\text{指向 } x<em>t \text{ 的修正方向}} + \underbrace{\sigma_t \epsilon_t}</em>{\text{随机噪声}}$  </p>
<h3 id="（三）Step-3：动机的实现（为什么能加速？）"><a href="#（三）Step-3：动机的实现（为什么能加速？）" class="headerlink" title="（三）Step 3：动机的实现（为什么能加速？）"></a>（三）Step 3：动机的实现（为什么能加速？）</h3><ul>
<li><strong>跳步（Sub - sampling）</strong>：DDPM 中 $x<em>{t - 1}$ 依赖 $x_t$；DDIM 公式可将 $t - 1$ 替换为任意更小时刻 $\tau$，支持从 $x</em>{1000}$ 直接推导 $x_{900}$，跳过中间步骤。  </li>
<li><strong>确定性（DDIM 核心）</strong>：令 $\sigma_t = 0$ 时，随机噪声项消失，$x_T$（纯噪声）确定则生成轨迹确定，扩散模型等价于求解常微分方程（ODE）。  </li>
</ul>
<h2 id="四、总结：两者的本质差异对比"><a href="#四、总结：两者的本质差异对比" class="headerlink" title="四、总结：两者的本质差异对比"></a>四、总结：两者的本质差异对比</h2><div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>DDPM</th>
<th>DDIM</th>
</tr>
</thead>
<tbody>
<tr>
<td>推导基石</td>
<td>马尔可夫链的贝叶斯推导</td>
<td>满足边缘分布一致性的非马尔可夫构造</td>
</tr>
<tr>
<td>采样公式</td>
<td>DDIM 中 $\sigma_t$ 取特定值的特例</td>
<td>泛化框架，调节 $\sigma_t$ 控随机性</td>
</tr>
<tr>
<td>数学形态</td>
<td>随机微分方程 (SDE)</td>
<td>$\sigma_t = 0$ 时为常微分方程 (ODE)</td>
</tr>
<tr>
<td>实用价值</td>
<td>奠定生成质量，推理慢</td>
<td>开启快速采样与图像编辑大门</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h1 id="一些更详细的推导"><a href="#一些更详细的推导" class="headerlink" title="一些更详细的推导"></a>一些更详细的推导</h1><p>这是一份经过严格数学逻辑梳理的 DDPM 与 DDIM 核心架构总结。我们不仅关注公式本身，更关注每一个公式背后的逻辑必然性。  </p>
<h2 id="1-DDPM-核心推导：变分下界（-L-vlb-）的完整脉络"><a href="#1-DDPM-核心推导：变分下界（-L-vlb-）的完整脉络" class="headerlink" title="1. DDPM 核心推导：变分下界（$L_{vlb}$）的完整脉络"></a>1. DDPM 核心推导：变分下界（$L_{vlb}$）的完整脉络</h2><p>DDPM 的目标是最小化负对数似然 $-\log p<em>\theta(x_0)$。由于直接计算边缘分布不可行，我们推导其变分上界（即变分下界 $L</em>{vlb}$ 的负数）。  </p>
<h3 id="Step-1-引入变分推断"><a href="#Step-1-引入变分推断" class="headerlink" title="Step 1: 引入变分推断"></a>Step 1: 引入变分推断</h3><p>利用 Jensen 不等式，将目标函数展开为路径上的联合分布：<br>$<br>-\log p<em>\theta(x_0) \le \mathbb{E}</em>{q(x<em>{1:T}|x_0)} \left[ \log \frac{q(x</em>{1:T}|x<em>0)}{p</em>\theta(x<em>{0:T})} \right] = L</em>{vlb}<br>$  </p>
<h3 id="Step-2-展开与分解"><a href="#Step-2-展开与分解" class="headerlink" title="Step 2: 展开与分解"></a>Step 2: 展开与分解</h3><p>根据马尔可夫链性质，$q(x<em>{1:T}|x_0) = \prod</em>{t=1}^T q(x<em>t|x</em>{t-1})$ 且 $p<em>\theta(x</em>{0:T}) = p(x<em>T) \prod</em>{t=1}^T p<em>\theta(x</em>{t-1}|x_t)$。</p>
<p>代入上式并利用贝叶斯公式 $q(x<em>t|x</em>{t-1}, x<em>0) = \frac{q(x</em>{t-1}|x<em>t, x_0)q(x_t|x_0)}{q(x</em>{t-1}|x_0)}$ 进行重组（此处省略项对消的代数过程）：  </p>
<script type="math/tex; mode=display">
L_{vlb} = \mathbb{E}_q \left[ 
\underbrace{D_{KL}(q(x_T|x_0) \| p(x_T))}_{L_T} 
+ \sum_{t=2}^T \underbrace{D_{KL}(q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t))}_{L_{t-1}} 
\underbrace{- \log p_\theta(x_0|x_1)}_{L_0} 
\right]</script><ul>
<li>$L_T$：前向扩散终点与先验噪声的距离，由于前向过程固定，该项为常数。  </li>
<li>$L<em>{t-1}$：核心训练项。它要求模型 $p</em>\theta(x<em>{t-1}|x_t)$ 去拟合以 $x_0$ 为条件的后验分布 $q(x</em>{t-1}|x_t, x_0)$。  </li>
</ul>
<h3 id="Step-3-核心推导动机"><a href="#Step-3-核心推导动机" class="headerlink" title="Step 3: 核心推导动机"></a>Step 3: 核心推导动机</h3><p>为什么 $L_{t-1}$ 变得可计算了？  </p>
<p>因为在给定 $x<em>0$ 时，$q(x</em>{t-1}|x_t, x_0)$ 具有闭式解（高斯分布）。通过计算两个高斯分布的 KL 散度，DDPM 将生成问题转化为了噪声预测问题：  </p>
<p>$<br>L<em>{simple} = \mathbb{E}</em>{x<em>0, \epsilon, t} \left[ | \epsilon - \epsilon</em>\theta(x_t, t) |^2 \right]<br>$  </p>
<hr>
<h3 id="2-DDIM-核心推导：基于“待定系数法”重构后验分布"><a href="#2-DDIM-核心推导：基于“待定系数法”重构后验分布" class="headerlink" title="2. DDIM 核心推导：基于“待定系数法”重构后验分布"></a>2. DDIM 核心推导：基于“待定系数法”重构后验分布</h3><p>DDIM 的核心洞察是：<strong>DDPM 的训练目标只依赖于边缘分布 $q(x_t|x_0)$</strong>。</p>
<blockquote>
<p><strong>为什么只依赖边缘分布？</strong><br>回顾 DDPM 的损失函数 $L<em>{\text{simple}} = \mathbb{E}</em>{x<em>0, \epsilon} [| \epsilon - \epsilon</em>\theta(x<em>t, t) |^2]$，其中输入的 $x_t$ 是通过 $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$ 得到的。可以看出，计算 Loss 仅需知道 $t$ 时刻 $x_t$ 相对于 $x_0$ 的分布（即边缘分布），而不需要知道 $x_t$ 究竟是经过怎样的路径（是否通过马尔可夫链）生成的（后面有进一步解释）。<br><strong>这意味着：</strong> 只要我们构造一个新的前向过程，保证其边缘分布与 DDPM 一致，就能直接复用训练好的 $\epsilon</em>\theta$ 模型，这为我们重新设计采样路径提供了理论自由度。</p>
</blockquote>
<h4 id="Step-1-明确已知条件与约束"><a href="#Step-1-明确已知条件与约束" class="headerlink" title="Step 1: 明确已知条件与约束"></a>Step 1: 明确已知条件与约束</h4><p>我们需要构造的分布必须满足以下两个核心约束（为了保证能复用 DDPM 训练好的模型）：</p>
<ol>
<li><strong>边缘分布守恒</strong>：任意时刻 $t$，数据必须满足高斯分布：<script type="math/tex; mode=display">x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I})</script></li>
<li><strong>时刻 $t-1$ 的一致性</strong>：同理，时刻 $t-1$ 也必须满足：<script type="math/tex; mode=display">x_{t-1} = \sqrt{\bar{\alpha}_{t-1}}x_0 + \sqrt{1-\bar{\alpha}_{t-1}}\epsilon', \quad \epsilon' \sim \mathcal{N}(0, \mathbf{I})</script></li>
</ol>
<h4 id="Step-2-待定系数法构建-x-t-1"><a href="#Step-2-待定系数法构建-x-t-1" class="headerlink" title="Step 2: 待定系数法构建 $x_{t-1}$"></a>Step 2: 待定系数法构建 $x_{t-1}$</h4><p>为了建立 $x_{t-1}$ 与 $x_t$ 的联系，我们观察到 $x_t$ 可以唯一确定当前时刻的累积噪声 $\epsilon_t$（在给定 $x_0$ 时）：</p>
<script type="math/tex; mode=display">\epsilon_t = \frac{x_t - \sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}}</script><p>我们可以假设生成的 $x<em>{t-1}$ 是由三部分组成的线性组合：<strong>“信号部分”</strong> ($x_0$)、<strong>“已知噪声部分”</strong> ($\epsilon_t$) 和 <strong>“新引入的随机噪声”</strong> ($\epsilon</em>{new}$)。</p>
<p><strong>设定待定系数方程：</strong></p>
<script type="math/tex; mode=display">x_{t-1} = \underbrace{C_1 \cdot x_0}_{\text{信号分量}} + \underbrace{C_2 \cdot \epsilon_t}_{\text{沿用时刻}t\text{的噪声}} + \underbrace{\sigma_t \cdot \epsilon_{new}}_{\text{独立随机噪声}}</script><p>其中 $\epsilon_{new} \sim \mathcal{N}(0, \mathbf{I})$，$\sigma_t$ 是我们手动引入控制随机性的超参数。</p>
<p><strong>求解系数 $C_1$ 和 $C_2$：</strong><br>我们要让上式满足 Step 1 中的边缘分布定义 $x<em>{t-1} \sim \mathcal{N}(\sqrt{\bar{\alpha}</em>{t-1}}x<em>0, (1-\bar{\alpha}</em>{t-1})\mathbf{I})$。</p>
<ol>
<li><p><strong>匹配均值（确定 $C_1$）</strong>：<br>对 $x_{t-1}$ 取期望（给定 $x_0$），噪声项均值为 0：</p>
<script type="math/tex; mode=display">\mathbb{E}[x_{t-1}|x_0] = C_1 x_0</script><p>根据边缘分布定义，均值应为 $\sqrt{\bar{\alpha}<em>{t-1}} x_0$。<br>$\therefore C_1 = \sqrt{\bar{\alpha}</em>{t-1}}$</p>
</li>
<li><p><strong>匹配方差（确定 $C_2$）</strong>：<br>计算 $x<em>{t-1}$ 的方差。由于 $\epsilon_t$ 和 $\epsilon</em>{new}$ 相互独立，方差具有可加性：</p>
<script type="math/tex; mode=display">\text{Var}[x_{t-1}|x_0] = C_2^2 + \sigma_t^2</script><p>根据边缘分布定义，总方差应为 $1 - \bar{\alpha}<em>{t-1}$。<br>$\therefore C_2^2 = (1 - \bar{\alpha}</em>{t-1}) - \sigma<em>t^2 \implies C_2 = \sqrt{1 - \bar{\alpha}</em>{t-1} - \sigma_t^2}$</p>
</li>
</ol>
<h4 id="Step-3-得到最终生成公式"><a href="#Step-3-得到最终生成公式" class="headerlink" title="Step 3: 得到最终生成公式"></a>Step 3: 得到最终生成公式</h4><p>将解出的系数代回原方程，我们得到了 $x<em>{t-1}$ 的解析形式（这也是 $q(x</em>{t-1}|x_t, x_0)$ 的采样实现）：</p>
<script type="math/tex; mode=display">x_{t-1} = \underbrace{\sqrt{\bar{\alpha}_{t-1}} x_0}_{\text{来自}x_0\text{的信号}} + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \cdot \epsilon_t}_{\text{指向}x_t\text{的方向}} + \underbrace{\sigma_t \cdot \epsilon_{new}}_{\text{随机扰动}}</script><p>在实际采样（反向过程）中，<strong>$x_0$ 是未知的</strong>，也无法获取真实的 $\epsilon<em>t$。因此，我们利用训练好的神经网络 $\epsilon</em>\theta(x_t, t)$ 来预测噪声，并推导出预测的 $x_0$（记为 $\hat{x}_0$）：</p>
<ol>
<li><strong>估计噪声</strong>：$\epsilon<em>t \approx \epsilon</em>\theta(x_t, t)$</li>
<li><strong>估计原图</strong>：$\hat{x}<em>0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon</em>\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}$</li>
</ol>
<p>将 $\hat{x}<em>0$ 和 $\epsilon</em>\theta$ 代入 Step 3 的公式，即得到 <strong>DDIM 最终采样公式</strong>：</p>
<script type="math/tex; mode=display">x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t)}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \cdot \epsilon_\theta(x_t) + \sigma_t \epsilon_{new}</script><p><strong>通过这种推导，我们可以清晰地看到：</strong></p>
<ul>
<li>DDPM 是上述公式中 $\sigma_t^2$ 取最大值（即完全匹配马尔可夫链方差）时的特例。</li>
<li>DDIM 是令 $\sigma_t=0$ 时的特例，此时随机项消失，采样变为确定的线性组合。</li>
</ul>
<h4 id="最后的说明——为什么：计算-Loss-仅需知道-t-时刻-x-t-相对于-x-0-的分布（即边缘分布），而不需要知道-x-t-究竟是经过怎样的路径（是否通过马尔可夫链）生成的"><a href="#最后的说明——为什么：计算-Loss-仅需知道-t-时刻-x-t-相对于-x-0-的分布（即边缘分布），而不需要知道-x-t-究竟是经过怎样的路径（是否通过马尔可夫链）生成的" class="headerlink" title="最后的说明——为什么：计算 Loss 仅需知道 $t$ 时刻 $x_t$ 相对于 $x_0$ 的分布（即边缘分布），而不需要知道 $x_t$ 究竟是经过怎样的路径（是否通过马尔可夫链）生成的?"></a>最后的说明——为什么：计算 Loss 仅需知道 $t$ 时刻 $x_t$ 相对于 $x_0$ 的分布（即边缘分布），而不需要知道 $x_t$ 究竟是经过怎样的路径（是否通过马尔可夫链）生成的?</h4><p>简单来说，原因在于：<strong>我们在训练时，是直接“跳”到 $t$ 时刻生成 $x_t$ 的，根本没有通过马尔可夫链一步步走过去。</strong></p>
<p>以下是详细的逻辑拆解：</p>
<h5 id="1-训练数据的构造方式：直接采样，而非递归生成"><a href="#1-训练数据的构造方式：直接采样，而非递归生成" class="headerlink" title="1. 训练数据的构造方式：直接采样，而非递归生成"></a>1. 训练数据的构造方式：直接采样，而非递归生成</h5><p>在 DDPM 的训练代码（以及算法原理）中，为了获得训练样本 $x_t$，我们并没有执行 $x_0 \to x_1 \to x_2 \dots \to x_t$ 这样的 $t$ 次加噪步骤。</p>
<p>相反，利用高斯分布的可加性，我们使用了一个<strong>“一步直达”</strong>的公式（即边缘分布公式）：</p>
<script type="math/tex; mode=display">x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I})</script><p><strong>这意味着：</strong></p>
<ul>
<li>当你计算 Loss 时，计算机做的事情是：随机取一张图 $x_0$，随机取一个时间 $t$，随机取一个噪声 $\epsilon$，直接通过上述公式算出 $x_t$。</li>
<li>在这个瞬间，<strong>中间状态 $x<em>1, x_2, \dots, x</em>{t-1}$ 在计算图中根本不存在</strong>。</li>
</ul>
<h5 id="2-神经网络的视角：只看当前，不问过往"><a href="#2-神经网络的视角：只看当前，不问过往" class="headerlink" title="2. 神经网络的视角：只看当前，不问过往"></a>2. 神经网络的视角：只看当前，不问过往</h5><p>再看 Loss 函数的具体计算过程：</p>
<script type="math/tex; mode=display">L = \| \epsilon - \epsilon_\theta(x_t, t) \|^2</script><p>神经网络 $\epsilon_\theta$ 接收的输入只有两个：</p>
<ol>
<li><strong>当前的状态</strong> $x_t$</li>
<li><strong>当前的时间</strong> $t$</li>
</ol>
<p>神经网络的任务是：“看着这张 $t$ 时刻的噪声图，猜猜刚才加了多少噪（$\epsilon$）”。</p>
<p><strong>关键点在于</strong>：只要 $x_t$ 的分布是正确的（即满足 $q(x_t|x_0)$），神经网络就能学会去噪。至于这个 $x_t$ 到底是怎么来的——是严格遵循马尔可夫链一步步加噪来的，还是像 DDIM 那样通过其他非马尔可夫路径来的，甚至是直接用公式算出来的——<strong>神经网络完全不知道，也不在乎</strong>。</p>
<h5 id="3-数学上的独立性（目标函数的解耦）"><a href="#3-数学上的独立性（目标函数的解耦）" class="headerlink" title="3. 数学上的独立性（目标函数的解耦）"></a>3. 数学上的独立性（目标函数的解耦）</h5><p>虽然 DDPM 的 Loss 推导最初是从联合分布 $q(x_{1:T}|x_0)$ 的变分下界（ELBO）开始的：</p>
<script type="math/tex; mode=display">L_{\text{vlb}} \approx \sum_{t=1}^T D_{KL}(q(x_{t-1}|x_t, x_0) || p_\theta(x_{t-1}|x_t))</script><p>但经过推导化简后，最终的 $L_{\text{simple}}$ 变成了对每个时刻 $t$ 的独立期望：</p>
<script type="math/tex; mode=display">L_{\text{simple}} = \sum_{t=1}^T \mathbb{E}_{q(x_t|x_0)} \left[ \| \dots \|^2 \right]</script><p>注意这里的期望 $\mathbb{E}$ 下标变成了 <strong>$q(x_t|x_0)$</strong>。这意味着，优化 $t$ 时刻的 Loss，只取决于 $t$ 时刻的边缘分布。各个时刻 $t$ 之间的依赖关系在 Loss 函数的最终形式中被<strong>解耦</strong>了。</p>
<hr>
<h2 id="3-DDIM-为什么能“跳步”？"><a href="#3-DDIM-为什么能“跳步”？" class="headerlink" title="3. DDIM 为什么能“跳步”？"></a>3. DDIM 为什么能“跳步”？</h2><h3 id="DDIM-跳步公式"><a href="#DDIM-跳步公式" class="headerlink" title="DDIM 跳步公式"></a>DDIM 跳步公式</h3><p>定义一个子序列 $\tau = [\tau_1, \tau_2, \dots, \tau_S]$，其中 $S \ll T$。采样公式改写为：  </p>
<p>$<br>x<em>{\tau</em>{i-1}} = \sqrt{\bar{\alpha}<em>{\tau</em>{i-1}}} \hat{x}<em>0 + \sqrt{1-\bar{\alpha}</em>{\tau<em>{i-1}}-\sigma</em>{\tau<em>i}^2} \epsilon</em>\theta(x<em>{\tau_i}, \tau_i) + \sigma</em>{\tau_i} \epsilon<br>$  </p>
<h3 id="核心原因分析：为什么-DDIM-能跳而-DDPM-不能？"><a href="#核心原因分析：为什么-DDIM-能跳而-DDPM-不能？" class="headerlink" title="核心原因分析：为什么 DDIM 能跳而 DDPM 不能？"></a>核心原因分析：为什么 DDIM 能跳而 DDPM 不能？</h3><h4 id="核心原因：概率依赖的解耦"><a href="#核心原因：概率依赖的解耦" class="headerlink" title="核心原因：概率依赖的解耦"></a>核心原因：概率依赖的解耦</h4><ul>
<li><strong>DDPM 的局限</strong>：DDPM 的采样逻辑建立在反向马尔可夫链 $p(x_{t-1}|x_t)$ 上。这个分布只有在步长极小（$\beta_t \to 0$）时，才能被近似为高斯分布。如果你强行跳步（比如从 $t=1000$ 跳到 $t=500$），步长过大，高斯近似失效，生成质量会崩溃。  </li>
<li><strong>DDIM 的突破</strong>：DDIM 重新定义的 $q(x_{t-1}|x_t, x_0)$ 本质上不是通过相邻步的近似得到的，而是通过强制匹配全局边缘分布 $q(x_t|x_0)$ 构造出来的。这意味着无论 $t$ 和 $t-1$ 之间跨度有多大，这个数学关系在定义上永远成立。  </li>
</ul>
<h4 id="确定性轨迹-vs-概率漂移"><a href="#确定性轨迹-vs-概率漂移" class="headerlink" title="确定性轨迹 vs 概率漂移"></a>确定性轨迹 vs 概率漂移</h4><p>DDIM 在 $\sigma_t=0$ 时变成了一个确定性的常微分方程（ODE）求解过程。在 ODE 框架下，跨步采样本质上就是更粗粒度的数值积分（如欧拉法），虽然会有截断误差，但不会像 DDPM 那样因为概率分布不匹配而导致“迷失方向”。  </p>
<h2 id="4-总结对比"><a href="#4-总结对比" class="headerlink" title="4. 总结对比"></a>4. 总结对比</h2><div class="table-container">
<table>
<thead>
<tr>
<th>特性</th>
<th>DDPM</th>
<th>DDIM</th>
</tr>
</thead>
<tbody>
<tr>
<td>数学基础</td>
<td>马尔可夫链 + 贝叶斯后验近似</td>
<td>非马尔可夫构造 + 边缘分布匹配</td>
</tr>
<tr>
<td>采样公式项</td>
<td>均值（包含当前噪声）+ 固定方差噪声</td>
<td>预测 $x_0$ 项 + 修正方向项 + 可变噪声项</td>
</tr>
<tr>
<td>跳步能力</td>
<td>不支持（受限于马尔可夫高斯近似）</td>
<td>支持（解耦了步间依赖，满足全局一致性）</td>
</tr>
<tr>
<td>生成性质</td>
<td>随机性随机演化</td>
<td>确定性映射（当 $\sigma_t=0$）</td>
</tr>
</tbody>
</table>
</div>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/26/cuda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/26/cuda/" class="post-title-link" itemprop="url">CUDA / CUDA Toolkit / PyTorch-CUDA 的关系与兼容性说明</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-26 10:00:00 / Modified: 13:32:16" itemprop="dateCreated datePublished" datetime="2025-12-26T10:00:00+08:00">2025-12-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDA-CUDA-Toolkit-PyTorch-CUDA-的关系与兼容性说明"><a href="#CUDA-CUDA-Toolkit-PyTorch-CUDA-的关系与兼容性说明" class="headerlink" title="CUDA / CUDA Toolkit / PyTorch-CUDA 的关系与兼容性说明"></a>CUDA / CUDA Toolkit / PyTorch-CUDA 的关系与兼容性说明</h1><p>本文按 <strong>概念区分 → 分层关系 → 兼容性规则 → 内在原因 → 常见误区速查</strong> 的逻辑，系统说明<br><strong>CUDA、CUDA Toolkit、PyTorch-CUDA 以及相关“带 CUDA 名字”的组件之间的关系与兼容要求</strong>。</p>
<hr>
<h2 id="一、核心名词与概念区分"><a href="#一、核心名词与概念区分" class="headerlink" title="一、核心名词与概念区分"></a>一、核心名词与概念区分</h2><h3 id="1-CUDA（抽象层）"><a href="#1-CUDA（抽象层）" class="headerlink" title="1. CUDA（抽象层）"></a>1. CUDA（抽象层）</h3><p><strong>CUDA 是 NVIDIA 提供的一套 GPU 并行计算平台与编程模型</strong></p>
<ul>
<li>并非一个可安装的软件包</li>
<li>定义了：<ul>
<li>并行执行模型（thread / warp / block / grid）</li>
<li>内存层级（global / shared / register / constant）</li>
<li>Host–Device 协作方式</li>
</ul>
</li>
<li>为后续所有 CUDA 实现（Toolkit、库、框架）提供“规范”</li>
</ul>
<blockquote>
<p>可以把 CUDA 理解为 <strong>ISA + 编程模型级别的标准</strong></p>
</blockquote>
<hr>
<h3 id="2-CUDA-Toolkit（开发工具集）"><a href="#2-CUDA-Toolkit（开发工具集）" class="headerlink" title="2. CUDA Toolkit（开发工具集）"></a>2. CUDA Toolkit（开发工具集）</h3><p><strong>CUDA Toolkit 是 CUDA 规范的具体实现与开发工具集合</strong></p>
<p>主要包含：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>组件</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>nvcc</code></td>
<td>CUDA 编译器</td>
</tr>
<tr>
<td>CUDA Runtime API</td>
<td>如 <code>cudaMalloc</code>、<code>cudaLaunchKernel</code></td>
</tr>
<tr>
<td>CUDA Driver API</td>
<td>更底层接口</td>
</tr>
<tr>
<td>cuBLAS / cuDNN / cuFFT</td>
<td>数值与深度学习库</td>
</tr>
<tr>
<td>Header / samples / profiler</td>
<td>开发与调试支持</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>CUDA Toolkit 面向 <strong>开发阶段</strong>：写 CUDA、编译 CUDA、调试 CUDA</p>
</blockquote>
<hr>
<h3 id="3-NVIDIA-Driver（驱动层，最底层）"><a href="#3-NVIDIA-Driver（驱动层，最底层）" class="headerlink" title="3. NVIDIA Driver（驱动层，最底层）"></a>3. NVIDIA Driver（驱动层，最底层）</h3><p><strong>NVIDIA Driver 是操作系统与 GPU 硬件之间的桥梁</strong></p>
<ul>
<li>实现 CUDA Driver API</li>
<li>负责：<ul>
<li>GPU 指令下发</li>
<li>上下文与内存管理</li>
<li>PTX → SASS 的 JIT 编译</li>
</ul>
</li>
<li>是 <strong>所有 CUDA 程序运行的必要条件</strong></li>
</ul>
<blockquote>
<p>⚠️ <code>nvidia-smi</code> 中显示的 <strong>CUDA Version</strong> 就来源于 <strong>Driver</strong><br>它表示：<strong>该驱动所支持的最高 CUDA 运行时版本</strong>，而不是你安装的 CUDA Toolkit 版本</p>
</blockquote>
<hr>
<h3 id="4-PyTorch-CUDA（框架运行时发行版）"><a href="#4-PyTorch-CUDA（框架运行时发行版）" class="headerlink" title="4. PyTorch-CUDA（框架运行时发行版）"></a>4. PyTorch-CUDA（框架运行时发行版）</h3><p><strong>PyTorch-CUDA 是“带 CUDA 支持的 PyTorch 二进制版本”</strong></p>
<p>典型形式：</p>
<ul>
<li><code>torch==2.2.0+cu118</code></li>
<li><code>torch==1.12.1+cu116</code></li>
</ul>
<p>其特点：</p>
<ul>
<li>已在指定 CUDA Toolkit 版本下编译</li>
<li>内置 CUDA Runtime + cuDNN / cuBLAS（通常）</li>
<li><strong>运行时只依赖 NVIDIA Driver</strong></li>
<li>不要求用户安装对应版本的 CUDA Toolkit</li>
</ul>
<blockquote>
<p>PyTorch-CUDA ≠ CUDA Toolkit<br>它是“<strong>已经编译好、可直接运行的 CUDA 应用</strong>”</p>
</blockquote>
<hr>
<h3 id="5-cuDNN-cuBLAS-NCCL（CUDA-上层库）"><a href="#5-cuDNN-cuBLAS-NCCL（CUDA-上层库）" class="headerlink" title="5. cuDNN / cuBLAS / NCCL（CUDA 上层库）"></a>5. cuDNN / cuBLAS / NCCL（CUDA 上层库）</h3><p>这些库：</p>
<ul>
<li>构建在 CUDA 之上</li>
<li>针对特定任务高度优化</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>库</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>cuDNN</td>
<td>深度学习算子</td>
</tr>
<tr>
<td>cuBLAS</td>
<td>线性代数</td>
</tr>
<tr>
<td>NCCL</td>
<td>多 GPU 通信</td>
</tr>
</tbody>
</table>
</div>
<p>在 PyTorch 中通常：</p>
<ul>
<li>已随 <code>torch+cuXXX</code> 打包</li>
<li>不需要单独安装</li>
</ul>
<hr>
<h2 id="二、整体分层关系（核心结构）"><a href="#二、整体分层关系（核心结构）" class="headerlink" title="二、整体分层关系（核心结构）"></a>二、整体分层关系（核心结构）</h2><p>┌────────────────────────────┐<br>│   PyTorch / TensorFlow     │  ← 应用 / 框架层<br>├────────────────────────────┤<br>│ PyTorch-CUDA (cu118 等)    │  ← 框架 + CUDA runtime<br>├────────────────────────────┤<br>│ CUDA Runtime / cuDNN 等    │  ← Toolkit 的运行时部分<br>├────────────────────────────┤<br>│ CUDA Driver API            │<br>├────────────────────────────┤<br>│ NVIDIA Driver              │  ← 必须存在<br>├────────────────────────────┤<br>│ GPU Hardware (SM 架构)     │<br>└────────────────────────────┘</p>
<p><strong>核心事实</strong>：</p>
<ul>
<li>Driver 在最底层</li>
<li>PyTorch-CUDA 自带 runtime</li>
<li>Toolkit 只在“开发 / 编译”阶段强依赖</li>
</ul>
<hr>
<h2 id="三、兼容性规则（实践中最重要）"><a href="#三、兼容性规则（实践中最重要）" class="headerlink" title="三、兼容性规则（实践中最重要）"></a>三、兼容性规则（实践中最重要）</h2><h3 id="规则-1：Driver-对-CUDA-版本-向后兼容"><a href="#规则-1：Driver-对-CUDA-版本-向后兼容" class="headerlink" title="规则 1：Driver 对 CUDA 版本 向后兼容"></a>规则 1：Driver 对 CUDA 版本 <strong>向后兼容</strong></h3><p>NVIDIA Driver 支持一个 <strong>最高 CUDA 版本</strong>（即 <code>nvidia-smi</code> 中显示的版本）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Driver 支持的 CUDA</th>
<th>可运行的 CUDA Runtime / PyTorch</th>
</tr>
</thead>
<tbody>
<tr>
<td>CUDA 12.2</td>
<td>12.2 / 12.1 / 11.8 / 11.6</td>
</tr>
<tr>
<td>CUDA 11.8</td>
<td>11.8 / 11.7 / 11.6</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p><code>nvidia-smi</code> 显示的 CUDA 版本 ≠ 本地安装的 CUDA Toolkit<br>它只说明 <strong>驱动“最多能理解到哪个 CUDA 版本”</strong></p>
</blockquote>
<hr>
<h3 id="规则-2：PyTorch-CUDA-只要求-Driver-够新"><a href="#规则-2：PyTorch-CUDA-只要求-Driver-够新" class="headerlink" title="规则 2：PyTorch-CUDA 只要求 Driver 够新"></a>规则 2：PyTorch-CUDA 只要求 Driver 够新</h3><p>例如：</p>
<p>```text<br>torch==2.2.0+cu118</p>
<p>运行条件：<br>    •    ✅ NVIDIA Driver ≥ CUDA 11.8<br>    •    ❌ 不要求本机安装 CUDA 11.8 Toolkit</p>
<p>这也是为什么：<br>    •    Conda / pip 安装即可直接用 GPU<br>    •    Docker 中常常“没装 CUDA Toolkit 也能跑”</p>
<p>⸻</p>
<p>规则 3：编译 CUDA 代码时，Toolkit 版本必须对齐</p>
<p>以下场景 必须安装 CUDA Toolkit：<br>    •    使用 nvcc<br>    •    编译 PyTorch CUDA extension<br>    •    从源码编译 PyTorch / Triton / 自定义算子</p>
<p>要求：<br>    •    CUDA Toolkit 版本 ≈ PyTorch 编译所用版本<br>    •    否则可能出现：<br>    •    ABI 不匹配<br>    •    undefined symbol<br>    •    运行期崩溃</p>
<p>⸻</p>
<p>规则 4：GPU 架构（SM）约束</p>
<p>每代 GPU 对应 Compute Capability（SM）</p>
<p>GPU    SM<br>V100    sm70<br>A100    sm80<br>H100    sm90</p>
<p>约束关系：<br>    •    新 Toolkit / PyTorch 可能 不再支持老 SM<br>    •    编译时是否包含目标 sm_xx</p>
<p>这是硬件指令集层面的限制，与软件无关</p>
<p>⸻</p>
<p>四、兼容性背后的内在原因</p>
<ol>
<li>CUDA 的执行模型</li>
</ol>
<p>CUDA 程序包含：<br>    •    PTX（中间表示）<br>    •    或 SASS（GPU 机器码）</p>
<p>Driver 负责：<br>    •    JIT 编译 PTX<br>    •    执行与调度</p>
<p>➡️ Driver 决定 “能否理解并执行该 CUDA 程序”</p>
<p>⸻</p>
<ol>
<li>ABI 并非完全稳定<br> •    CUDA Runtime / cuDNN 会演进<br> •    ABI 并非无限向前兼容<br> •    框架必须在固定 Toolkit 上编译</li>
</ol>
<p>➡️ 这导致 torch+cu116、torch+cu118 的存在</p>
<p>⸻</p>
<ol>
<li>PyTorch 选择“自带 CUDA runtime”的工程考量</li>
</ol>
<p>目的：<br>    •    降低用户安装复杂度<br>    •    提高环境可复现性</p>
<p>代价：<br>    •    版本命名复杂<br>    •    初学者容易混淆 CUDA / Toolkit / Driver</p>
<p>⸻</p>
<p>五、常见“带 CUDA 名字”的组件速查表</p>
<p>名称    本质    是否必须<br>NVIDIA Driver    硬件驱动    ✅ 必须<br>CUDA    编程模型 / 规范    ❌<br>CUDA Toolkit    开发工具集    ⭕ 编译时<br>PyTorch-CUDA    框架运行时    ✅（GPU 版）<br>cuDNN / cuBLAS    CUDA 上层库    ⭕（通常已打包）<br>nvcc    CUDA 编译器    ❌（除非编译）</p>
<p>⸻</p>
<p>六、一句话实战总结<br>    •    只训练模型：Driver + torch+cuXXX 即可<br>    •    写 / 编译 CUDA：Driver + 对齐版本的 CUDA Toolkit<br>    •    判断能不能跑：看 nvidia-smi 中 Driver 支持的 CUDA 是否 ≥ 运行时需求</p>
<p>nvidia-smi 显示的 CUDA 版本<br>本质是 Driver 能支持的最高 CUDA 运行时版本，而不是你“装了哪个 CUDA”</p>
<p>⸻</p>
<p>如果你需要，我可以在此基础上进一步整理：</p>
<ul>
<li><strong>Docker / Conda / 裸机 安装场景对照表</strong></li>
<li><strong>Driver × CUDA × PyTorch 的“最稳组合矩阵”</strong></li>
<li><strong>面试版：一句话区分 CUDA / Toolkit / Driver / Framework</strong></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/26/multimodal/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/26/multimodal/" class="post-title-link" itemprop="url">多模态学习资料汇总</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-26 10:00:00 / Modified: 13:32:22" itemprop="dateCreated datePublished" datetime="2025-12-26T10:00:00+08:00">2025-12-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?type=content&amp;q=%E5%A4%9A%E6%A8%A1%E6%80%81%20%E8%A7%A3%E8%AF%BB">https://www.zhihu.com/search?type=content&amp;q=%E5%A4%9A%E6%A8%A1%E6%80%81%20%E8%A7%A3%E8%AF%BB</a></p>
<p>SLIP DeCLIP BLIP ALBEF</p>
<hr>
<h1 id="多模态大模型-MLLM-核心研究进展全景综述"><a href="#多模态大模型-MLLM-核心研究进展全景综述" class="headerlink" title="多模态大模型 (MLLM) 核心研究进展全景综述"></a>多模态大模型 (MLLM) 核心研究进展全景综述</h1><h2 id="1-发展简史：从“特征匹配”到“世界理解”"><a href="#1-发展简史：从“特征匹配”到“世界理解”" class="headerlink" title="1. 发展简史：从“特征匹配”到“世界理解”"></a>1. 发展简史：从“特征匹配”到“世界理解”</h2><p>多模态技术的发展经历了从特定任务建模到通用智能的跨越，其演进脉络清晰地展现了模型“理解力”的提升：</p>
<ul>
<li><strong>判别式对齐时期 (2021-2022)：</strong> <strong>CLIP</strong> 的出现是里程碑，它通过大规模对比学习将图文映射到统一空间。随后 <strong>BLIP</strong>、<strong>ALBEF</strong> 等模型通过引入更复杂的损失函数（如 ITM, LM）进一步强化了细粒度对齐。</li>
<li><strong>生成式大模型时期 (2023-2024)：</strong> <strong>LLaVA</strong> 开创了将预训练 ViT 与开源 LLM（如 Llama）通过简单投影层对接的范式。<strong>InstructBLIP</strong> 等模型开始引入指令微调。</li>
<li><strong>高分辨率与原生多模态时期 (2024-2025)：</strong> 随着 <strong>GPT-4o</strong>、<strong>Qwen-VL</strong> 的发布，研究重点转向了 <strong>AnyRes</strong>（任意分辨率处理）和原生多模态架构，旨在解决视觉细节丢失和模态间深层融合的问题。</li>
</ul>
<hr>
<h2 id="2-视觉表示学习代表模型"><a href="#2-视觉表示学习代表模型" class="headerlink" title="2. 视觉表示学习代表模型"></a>2. 视觉表示学习代表模型</h2><p>这些模型作为 MLLM 的“感知器”，负责提供高质量的视觉特征：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>核心设计</th>
<th>优势与特性</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CLIP</strong></td>
<td>双塔架构 + 对比学习</td>
<td>极强的 Zero-shot 迁移能力。</td>
<td>图像分类、图文检索。</td>
</tr>
<tr>
<td><strong>SLIP / DeCLIP</strong></td>
<td>引入自监督信号</td>
<td>提高数据利用率，特征更具鲁棒性。</td>
<td>小规模数据集训练。</td>
</tr>
<tr>
<td><strong>ALBEF</strong></td>
<td>动量队列 + 跨模态注意力</td>
<td>解决了图文对中噪声数据的影响。</td>
<td>细粒度对齐任务。</td>
</tr>
<tr>
<td><strong>BLIP (早期)</strong></td>
<td>Encoder-Decoder 统一架构</td>
<td>能够通过自举（Bootstrapping）清洗网络噪声数据。</td>
<td>图像描述、图文匹配。</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="3-BLIP-的核心损失函数解析"><a href="#3-BLIP-的核心损失函数解析" class="headerlink" title="3. BLIP 的核心损失函数解析"></a>3. BLIP 的核心损失函数解析</h2><p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/627481137">https://zhuanlan.zhihu.com/p/627481137</a></p>
<p>BLIP (Bootstrapping Language-Image Pre-training) 的成功很大程度上归功于其联合优化的三个目标函数：</p>
<ol>
<li><strong>ITC (Image-Text Contrastive Loss)：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 类似于 CLIP，通过对比学习拉近配对的图文特征，推开不匹配的。</li>
<li><strong>作用：</strong> 学习全局的模态对齐。</li>
</ul>
<ol>
<li><strong>ITM (Image-Text Matching Loss)：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 一个二分类任务，预测给定的图片和文本是否真正匹配。</li>
<li><strong>作用：</strong> 强迫模型通过跨模态注意力（Cross-Attention）去捕捉局部细节，实现细粒度理解。</li>
</ul>
<ol>
<li><strong>LM (Language Modeling Loss)：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 给定图片，自回归地生成对应的文本描述。</li>
<li><strong>作用：</strong> 赋予模型生成能力，使其能够根据视觉线索产出连贯的自然语言。</li>
</ul>
<hr>
<h2 id="4-多模态大模型代表模型与技术差异"><a href="#4-多模态大模型代表模型与技术差异" class="headerlink" title="4. 多模态大模型代表模型与技术差异"></a>4. 多模态大模型代表模型与技术差异</h2><h3 id="A-标志性模型架构"><a href="#A-标志性模型架构" class="headerlink" title="A. 标志性模型架构"></a>A. 标志性模型架构</h3><ul>
<li><strong>LLaVA:</strong> 采用 <strong>Linear Projection（输入层拼接）</strong>。其核心贡献在于将多模态数据转化为“视觉单词”，直接喂给 LLM。</li>
<li><strong>Qwen-VL:</strong> 引入了 <strong>Visual Resampler</strong>。它能将高分辨率产生的数千个 Token 压缩为固定数量的有效 Token，平衡了细节与计算量。</li>
<li><strong>Flamingo:</strong> 使用 <strong>Gated Cross-Attention</strong>。在 LLM 的层间插入视觉注入层，保持了 LLM 原始的语言能力。</li>
</ul>
<h3 id="B-关键进阶技术：AnyRes-与-Deepstack"><a href="#B-关键进阶技术：AnyRes-与-Deepstack" class="headerlink" title="B. 关键进阶技术：AnyRes 与 Deepstack"></a>B. 关键进阶技术：AnyRes 与 Deepstack</h3><p>为了处理现实世界中复杂的视觉输入，以下技术成为了当前主流：</p>
<ul>
<li><strong>AnyRes (Any Resolution)：</strong></li>
<li><strong>核心逻辑：</strong> 传统的 ViT 通常只能处理固定的低分辨率输入（如 ）。AnyRes 技术（如在 LLaVA-NeXT 中应用）将原始高分辨率图片切分成多个子图（Patches）分别编码，并保留一个缩略总图提供全局信息。</li>
<li><strong>优势：</strong> 显著提升了模型对高清图像、小文字（OCR）和微小对象的识别能力。</li>
</ul>
<ul>
<li><strong>Deepstack (深度堆叠)：</strong></li>
<li><strong>核心逻辑：</strong> 这种技术旨在解决多模态特征在进入 LLM 后的“被稀释”问题。它通过在 LLM 的多个层级（而非仅输入层）重复注入或堆叠视觉特征。</li>
<li><strong>优势：</strong> 增强了视觉信息的持久性，确保模型在处理长文本回复时，末尾的内容依然能紧扣开头的视觉细节。</li>
</ul>
<hr>
<h2 id="5-常见训练方法归纳"><a href="#5-常见训练方法归纳" class="headerlink" title="5. 常见训练方法归纳"></a>5. 常见训练方法归纳</h2><ol>
<li><strong>预训练 (Pre-alignment)：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 冻结两头，只练中间。学习从视觉空间到语言空间的线性映射。</li>
</ul>
<ol>
<li><strong>指令微调 (Instruction Tuning)：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 使用高质量对话数据训练。重点在于让模型学会“回答格式”和“复杂逻辑推理”。</li>
</ul>
<ol>
<li><strong>多任务联合训练：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 同时喂入检测、分割、描述等任务。</li>
<li><strong>场景：</strong> 提升模型的定位（Grounding）能力，例如让模型能说出物体在图中的坐标。</li>
</ul>
<ol>
<li><strong>DPO (Direct Preference Optimization)：</strong></li>
</ol>
<ul>
<li><strong>逻辑：</strong> 针对多模态幻觉进行优化，让模型在“正确回答”和“虚假回答”中学会选择前者。</li>
</ul>
<hr>
<h2 id="6-技术演进规律提炼"><a href="#6-技术演进规律提炼" class="headerlink" title="6. 技术演进规律提炼"></a>6. 技术演进规律提炼</h2><ul>
<li><strong>分辨率革命：</strong> 从固定低像素到 <strong>AnyRes</strong> 动态切片。</li>
<li><strong>融合深度：</strong> 从简单的输入拼接（Projection）到层间深度堆叠（Deepstack/Cross-Attention）。</li>
<li><strong>数据范式：</strong> 从海量噪声数据（CLIP 时代）到精细化的人工指令数据（MLLM 时代）。</li>
</ul>
<hr>
<blockquote>
<p><strong>结语：</strong> 多模态大模型的研究正处于从“看图说话”向“视觉专家”转变的关键期。掌握 BLIP 的对齐机制是基础，而理解 AnyRes 等高阶视觉增强技术则是进阶当前前沿架构的关键。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/26/pycache/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/26/pycache/" class="post-title-link" itemprop="url">Python __pycache__ 目录详解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-26 10:00:00 / Modified: 13:32:24" itemprop="dateCreated datePublished" datetime="2025-12-26T10:00:00+08:00">2025-12-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>__pycache__</code> 是Python运行时自动生成的目录，用于存放编译后的字节码文件（<code>.pyc</code>/<code>.pyo</code>）。当首次执行<code>.py</code>文件时，Python会将源码编译为字节码（介于源码和机器码之间的中间代码），并将其缓存到<code>__pycache__</code>中，命名格式为<code>模块名.版本信息.pyc</code>（如<code>test.cpython-310.pyc</code>）。</p>
<p>核心作用是提升代码重复执行效率：后续运行时，Python会优先读取缓存的字节码，无需重新编译源码，尤其对大型项目或频繁调用的模块，能显著缩短启动时间。该目录由Python自动管理——修改源码后，Python会对比源码和字节码的时间戳，自动重新编译并更新缓存；删除目录也不影响代码运行，下次执行时会重新生成。</p>
<p><code>__pycache__</code>属于临时缓存，无需手动维护，通常会加入<code>.gitignore</code>避免提交到版本库；也可通过<code>python -B 脚本名</code>或设置<code>PYTHONDONTWRITEBYTECODE</code>环境变量，禁止生成该目录。它仅存于模块所在目录，不同Python版本生成的字节码不兼容，因此目录内会按版本区分文件。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li><code>__pycache__</code>是Python自动生成的字节码缓存目录，用于提升代码执行效率；</li>
<li>无需手动维护，可通过参数/环境变量禁止生成，建议加入版本库忽略列表；</li>
<li>字节码按Python版本区分，修改源码会自动更新缓存。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/25/VMvare%20and%20Contianer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/25/VMvare%20and%20Contianer/" class="post-title-link" itemprop="url">操作系统虚拟化：虚拟机和容器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-12-25 14:00:00" itemprop="dateCreated datePublished" datetime="2025-12-25T14:00:00+08:00">2025-12-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-26 13:28:52" itemprop="dateModified" datetime="2025-12-26T13:28:52+08:00">2025-12-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">操作系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="VMvare"><a href="#VMvare" class="headerlink" title="VMvare"></a>VMvare</h1><h2 id="大致总结"><a href="#大致总结" class="headerlink" title="大致总结"></a>大致总结</h2><h3 id="1-虚拟机（Virtual-Machine）的兴起与硬核实现"><a href="#1-虚拟机（Virtual-Machine）的兴起与硬核实现" class="headerlink" title="1. 虚拟机（Virtual Machine）的兴起与硬核实现"></a>1. 虚拟机（Virtual Machine）的兴起与硬核实现</h3><h4 id="1-1-虚拟机的基本思想：一切皆状态机"><a href="#1-1-虚拟机的基本思想：一切皆状态机" class="headerlink" title="1.1 虚拟机的基本思想：一切皆状态机"></a>1.1 虚拟机的基本思想：一切皆状态机</h4><p>课程从计算机系统的抽象视角出发，明确核心观点——“一切皆状态机（Everything is a State Machine）”。这一思想为虚拟化技术奠定了理论基础，具体内涵如下：</p>
<ul>
<li>计算机的执行状态由<strong>寄存器、内存、外设状态</strong>共同构成；</li>
<li>虚拟机的本质是<strong>对另一台计算机状态机的完整复现与控制</strong>；</li>
<li>只要能够精确维护并推进该状态机，即可在单台物理机上实现“另一台计算机”的运行。</li>
</ul>
<h4 id="1-2-早期实现：软件模拟与性能瓶颈"><a href="#1-2-早期实现：软件模拟与性能瓶颈" class="headerlink" title="1.2 早期实现：软件模拟与性能瓶颈"></a>1.2 早期实现：软件模拟与性能瓶颈</h4><h5 id="1-2-1-纯软件模拟（Emulation）"><a href="#1-2-1-纯软件模拟（Emulation）" class="headerlink" title="1.2.1 纯软件模拟（Emulation）"></a>1.2.1 纯软件模拟（Emulation）</h5><p>早期虚拟机系统的核心实现方式为<strong>全软件模拟</strong>，典型代表包括教学场景或调试工具中的模拟器（如NEMU），其技术特征如下：</p>
<ul>
<li>客户机（Guest）的每条指令均由宿主机（Host）程序解释执行；</li>
<li>需在软件层面完整模拟CPU、内存、I/O设备等计算机核心组件；</li>
<li>具备极高的可移植性与执行可控性。</li>
</ul>
<h5 id="1-2-2-性能瓶颈与核心问题"><a href="#1-2-2-性能瓶颈与核心问题" class="headerlink" title="1.2.2 性能瓶颈与核心问题"></a>1.2.2 性能瓶颈与核心问题</h5><p>纯软件模拟虽能满足功能需求，但存在不可忽视的性能缺陷：</p>
<ul>
<li>指令级的解释执行导致<strong>数量级的性能损失</strong>；</li>
<li>实际运行速度通常仅为原生系统的1%左右；</li>
<li>难以适配通用操作系统或生产环境的性能需求。</li>
</ul>
<p>这一现状引发关键思考：能否在不完全模拟硬件的前提下，实现接近原生系统的执行性能？</p>
<h4 id="1-3-VMware-的关键突破：软件虚拟化"><a href="#1-3-VMware-的关键突破：软件虚拟化" class="headerlink" title="1.3 VMware 的关键突破：软件虚拟化"></a>1.3 VMware 的关键突破：软件虚拟化</h4><h5 id="1-3-1-核心设计思想"><a href="#1-3-1-核心设计思想" class="headerlink" title="1.3.1 核心设计思想"></a>1.3.1 核心设计思想</h5><p>1998年成立的VMware提出了革命性的工程实现方案，其核心思想跳出“全模拟”框架：<strong>让虚拟机中的大多数代码直接在物理CPU上运行</strong>，仅对关键环节进行干预。具体包括：</p>
<ul>
<li>在宿主操作系统内核中加载<strong>虚拟化驱动模块</strong>；</li>
<li>对客户机的执行环境进行精细化控制；</li>
<li>仅在必要时（如特权操作执行）进行拦截与处理。</li>
</ul>
<h5 id="1-3-2-关键技术机制"><a href="#1-3-2-关键技术机制" class="headerlink" title="1.3.2 关键技术机制"></a>1.3.2 关键技术机制</h5><p>VMware通过以下核心技术实现性能突破：</p>
<ol>
<li><strong>地址空间重映射</strong><ul>
<li>“悄悄”修改进程的物理内存映射关系；</li>
<li>将虚拟机内的进程“迁移”至虚拟机外，以原生方式直接运行。</li>
</ul>
</li>
<li><strong>特权指令处理</strong><ul>
<li>普通指令无需拦截，直接在物理CPU上执行；</li>
<li>特权指令通过陷入（trap）或二进制翻译机制，交由虚拟机监控器（VMM）处理。</li>
</ul>
</li>
</ol>
<h5 id="1-3-3-技术里程碑意义"><a href="#1-3-3-技术里程碑意义" class="headerlink" title="1.3.3 技术里程碑意义"></a>1.3.3 技术里程碑意义</h5><p>该方案在<strong>无额外硬件支持</strong>的前提下，实现了三大突破：</p>
<ul>
<li>执行性能接近原生系统；</li>
<li>实现对完整操作系统的透明虚拟化；</li>
<li>推动虚拟机从“学术研究工具”走向“商业化可行产品”。</li>
</ul>
<p>这一突破标志着虚拟化技术正式从学术领域迈向工业基础设施。</p>
<h4 id="1-4-硬件虚拟化支持：Intel-VT-x-与-EPT"><a href="#1-4-硬件虚拟化支持：Intel-VT-x-与-EPT" class="headerlink" title="1.4 硬件虚拟化支持：Intel VT-x 与 EPT"></a>1.4 硬件虚拟化支持：Intel VT-x 与 EPT</h4><h5 id="1-4-1-VT-x：CPU级虚拟化原生支持"><a href="#1-4-1-VT-x：CPU级虚拟化原生支持" class="headerlink" title="1.4.1 VT-x：CPU级虚拟化原生支持"></a>1.4.1 VT-x：CPU级虚拟化原生支持</h5><p>为进一步简化虚拟化实现、提升性能与稳定性，Intel推出VT-x（Virtualization Technology）技术，其核心特性如下：</p>
<ul>
<li>在CPU硬件层面引入<strong>Guest/Host双执行模式</strong>；</li>
<li>当客户机执行敏感指令或特权指令时，自动触发VM Exit机制；</li>
<li>安全地将控制权转移至虚拟机监控器（Hypervisor）。</li>
</ul>
<p>VT-x的技术优势体现在：</p>
<ul>
<li>减少复杂的二进制翻译流程；</li>
<li>提升虚拟化系统的可靠性与可维护性；</li>
<li>显著降低虚拟机监控器（VMM）的实现复杂度。</li>
</ul>
<h5 id="1-4-2-EPT：扩展页表（Extended-Page-Tables）"><a href="#1-4-2-EPT：扩展页表（Extended-Page-Tables）" class="headerlink" title="1.4.2 EPT：扩展页表（Extended Page Tables）"></a>1.4.2 EPT：扩展页表（Extended Page Tables）</h5><p>EPT技术针对性解决了虚拟内存虚拟化中的地址转换核心问题。在虚拟化场景中，地址转换需经历三级链路：<br><code>客户机虚拟地址（GVA）→ 客户机物理地址（GPA）→ 宿主机物理地址（HPA）</code></p>
<p>EPT的引入实现了关键优化：</p>
<ul>
<li>三级地址转换由硬件自动完成，无需软件干预；</li>
<li>大幅减少页表维护的软件开销；</li>
</ul>
<hr>
<h2 id="为什么虚拟机修改CR3或者访问内存需要VMM管理？不能直接由虚拟机的内核管理吗，即让虚拟机自己维护一个CR3，虚拟机内核自己计算出GPA后，（因为虚拟机本身就是主机的一个进程），可以将这个GPA视为一个普通的VA，利用主机的地址转换机制得到HPA？"><a href="#为什么虚拟机修改CR3或者访问内存需要VMM管理？不能直接由虚拟机的内核管理吗，即让虚拟机自己维护一个CR3，虚拟机内核自己计算出GPA后，（因为虚拟机本身就是主机的一个进程），可以将这个GPA视为一个普通的VA，利用主机的地址转换机制得到HPA？" class="headerlink" title="为什么虚拟机修改CR3或者访问内存需要VMM管理？不能直接由虚拟机的内核管理吗，即让虚拟机自己维护一个CR3，虚拟机内核自己计算出GPA后，（因为虚拟机本身就是主机的一个进程），可以将这个GPA视为一个普通的VA，利用主机的地址转换机制得到HPA？"></a>为什么虚拟机修改CR3或者访问内存需要VMM管理？不能直接由虚拟机的内核管理吗，即让虚拟机自己维护一个CR3，虚拟机内核自己计算出GPA后，（因为虚拟机本身就是主机的一个进程），可以将这个GPA视为一个普通的VA，利用主机的地址转换机制得到HPA？</h2><p>总结来说，虚拟机不能直接管理 <code>CR3</code> 或自行完成地址转换，核心原因有三点：</p>
<ol>
<li><strong>权限与安全（隔离性）</strong>：<br>如果允许虚拟机内核直接控制 <code>CR3</code> 并在物理 CPU 上运行，它就能将虚拟地址映射到<strong>物理机内存的任何角落</strong>（包括宿主机内核或其他虚拟机的空间）。为了保证宿主机的安全，物理 <code>CR3</code> 的控制权必须由最高特权的 <strong>VMM (Root 模式)</strong> 掌握。</li>
<li><strong>物理地址冲突（认知偏差）</strong>：<br>虚拟机内核认为自己拥有从零开始的连续“物理内存”（GPA），但实际上它只是宿主机进程里的一块虚拟空间。如果没有 VMM 介入，虚拟机算出来的地址（GPA）直接丢给内存总线，会指向错误的、甚至不存在的物理硬件。</li>
<li><strong>硬件设计的局限（在 EPT 出现前）</strong>：<br>传统的 CPU 内存管理单元（MMU）是一次性映射的，它只认一套页表。它没有“先翻译一次（GVA→GPA），再把结果当成虚拟地址翻译第二次（GPA→HPA）”的功能。</li>
</ol>
<h3 id="演进结果"><a href="#演进结果" class="headerlink" title="演进结果"></a>演进结果</h3><ul>
<li><strong>早期方案（影子页表）</strong>：强制拦截。由于硬件太“笨”，VMM 必须拦截所有 <code>CR3</code> 操作，手工合成一套“影子页表”来骗过 CPU，让 CPU 直接跨过 GPA 这一层。</li>
<li><strong>现代方案（EPT）</strong>：硬件辅助。硬件在 MMU 里增加了<strong>第二层翻译逻辑</strong>。正如你所想，现在虚拟机确实可以自己管理一个“假 CR3”，而 CPU 硬件会自动在后台帮它完成第二步转换。</li>
</ul>
<p><strong>简而言之：不让它管是为了“防盗”（安全）和“防撞”（冲突），而现在能管是因为“硬件变聪明了”（EPT）。</strong></p>
<hr>
<h2 id="在实现虚拟机时，请分别举一个不使用和使用intel-VT-x的具体例子帮助我理解VT-x的作用；请举一个具体的使用扩展页表的例子帮助我理解扩展页表的作用？"><a href="#在实现虚拟机时，请分别举一个不使用和使用intel-VT-x的具体例子帮助我理解VT-x的作用；请举一个具体的使用扩展页表的例子帮助我理解扩展页表的作用？" class="headerlink" title="在实现虚拟机时，请分别举一个不使用和使用intel VT-x的具体例子帮助我理解VT-x的作用；请举一个具体的使用扩展页表的例子帮助我理解扩展页表的作用？"></a>在实现虚拟机时，请分别举一个不使用和使用intel VT-x的具体例子帮助我理解VT-x的作用；请举一个具体的使用扩展页表的例子帮助我理解扩展页表的作用？</h2><p>实现虚拟机时，硬件辅助虚拟化技术（如 Intel VT-x）极大地简化了监控程序（VMM/Hypervisor）的设计，并提升了性能。</p>
<p>以下通过对比和具体场景为你解释这两个核心概念：</p>
<h3 id="1-Intel-VT-x-的作用：从“模拟器”到“直通车”"><a href="#1-Intel-VT-x-的作用：从“模拟器”到“直通车”" class="headerlink" title="1. Intel VT-x 的作用：从“模拟器”到“直通车”"></a>1. Intel VT-x 的作用：从“模拟器”到“直通车”</h3><h4 id="不使用-VT-x-的例子：二进制翻译-Binary-Translation"><a href="#不使用-VT-x-的例子：二进制翻译-Binary-Translation" class="headerlink" title="不使用 VT-x 的例子：二进制翻译 (Binary Translation)"></a>不使用 VT-x 的例子：二进制翻译 (Binary Translation)</h4><p>在没有 VT-x 之前（如早期的 VMware 或现有的纯软件模拟器 QEMU），如果 Guest OS（虚拟机操作系统）想要执行一条<strong>特权指令</strong>（比如 <code>LGDT</code> 加载全局描述符表，或者修改控制寄存器 <code>MOV CR3, EAX</code>）：</p>
<ol>
<li><strong>无法直接运行</strong>：Guest OS 运行在非特权级（Ring 3 或 Ring 1），直接执行这些指令会触发 CPU 异常或静默失败（Silent Failure）。</li>
<li><strong>软件扫描与替换</strong>：VMM 必须像翻译员一样，在 Guest 代码运行前，逐行扫描二进制指令。</li>
<li><strong>动态修补</strong>：发现特权指令后，VMM 将其“挖掉”，替换成一段跳转到 VMM 内部的模拟代码。</li>
<li><strong>代价</strong>：这就像一边翻译外语一边看书，<strong>性能损耗巨大</strong>，且 VMM 逻辑极其复杂，必须处理指令集的各种边角案例。</li>
</ol>
<h4 id="使用-VT-x-的例子：硬件自动拦截-VM-Exit"><a href="#使用-VT-x-的例子：硬件自动拦截-VM-Exit" class="headerlink" title="使用 VT-x 的例子：硬件自动拦截 (VM-Exit)"></a>使用 VT-x 的例子：硬件自动拦截 (VM-Exit)</h4><p>有了 VT-x，CPU 引入了两种模式：<strong>Root 模式</strong>（VMM 运行）和 <strong>Non-Root 模式</strong>（Guest 运行）。</p>
<ol>
<li><strong>直接执行</strong>：Guest OS 可以直接在 CPU 上以原生的速度运行大部分指令。</li>
<li><strong>精准捕获</strong>：当 Guest OS 执行修改 <code>CR3</code> 的指令时，硬件会自动触发一个 <strong>VM-Exit</strong>。</li>
<li><strong>硬件保存现场</strong>：CPU 自动根据 <strong>VMCS</strong>（虚拟机控制结构）保存当前 Guest 的所有寄存器状态，并跳回 VMM。</li>
<li><strong>VMM 处理</strong>：VMM 检查 VMCS 中的退出原因，帮 Guest 修改一下映射，然后执行 <code>VMLAUNCH/VMRESUME</code> 让 CPU 回到 Guest 继续运行。</li>
<li><strong>作用</strong>：VT-x 让硬件承担了“监视和拦截”的工作，VMM 从“翻译员”变成了“管理员”。</li>
</ol>
<h3 id="2-扩展页表-EPT-的作用：消除“影子”"><a href="#2-扩展页表-EPT-的作用：消除“影子”" class="headerlink" title="2. 扩展页表 (EPT) 的作用：消除“影子”"></a>2. 扩展页表 (EPT) 的作用：消除“影子”</h3><p>在没有 EPT（Extended Page Tables）时，地址转换非常痛苦，因为存在<strong>两层映射</strong>：</p>
<ul>
<li><strong>GVA</strong> (Guest 虚拟地址)  <strong>GPA</strong> (Guest 物理地址)</li>
<li><strong>GPA</strong> (Guest 物理地址)  <strong>HPA</strong> (Host 物理地址)</li>
</ul>
<h4 id="具体例子：修改内存条目"><a href="#具体例子：修改内存条目" class="headerlink" title="具体例子：修改内存条目"></a>具体例子：修改内存条目</h4><p>假设你有一个虚拟机正在运行一个浏览器。浏览器申请了一块内存（GVA 为 <code>0x1234</code>），在 Guest 看来，这对应了它的内存条 <code>0x5678</code> 处（GPA）。但实际上，宿主机分配给这个飞地的真实内存是在 <code>0x9ABC</code>（HPA）。</p>
<h5 id="没有-EPT-时：影子页表-Shadow-Page-Tables"><a href="#没有-EPT-时：影子页表-Shadow-Page-Tables" class="headerlink" title="没有 EPT 时：影子页表 (Shadow Page Tables)"></a>没有 EPT 时：影子页表 (Shadow Page Tables)</h5><ol>
<li><strong>同步负担</strong>：VMM 必须在内核中维护一套“影子页表”，直接存储 <strong>GVA  HPA</strong>。</li>
<li><strong>频繁拦截</strong>：每当 Guest OS 修改自己的页表（比如进程切换修改 CR3），都会触发一次 VM-Exit。</li>
<li><strong>计算量</strong>：VMM 必须手动遍历 Guest 的页表，计算出对应的 HPA，填入影子页表。如果 Guest 频繁切换进程，VMM 就会忙于更新影子页表，导致系统卡顿。</li>
</ol>
<h5 id="使用-EPT-时：硬件级二级跳"><a href="#使用-EPT-时：硬件级二级跳" class="headerlink" title="使用 EPT 时：硬件级二级跳"></a>使用 EPT 时：硬件级二级跳</h5><p>有了 EPT，CPU 内部集成了两套页表遍历引擎。</p>
<ol>
<li><strong>硬件自动翻译</strong>：</li>
</ol>
<ul>
<li>第一阶段：CPU 先按 Guest 的页表把 <strong>GVA 转为 GPA</strong>。</li>
<li>第二阶段：CPU <strong>自动</strong>根据 VMM 设置好的 EPT 把 <strong>GPA 转为 HPA</strong>。</li>
</ul>
<ol>
<li><strong>零拦截</strong>：Guest OS 可以随意修改自己的页表（修改 CR3），而<strong>不会产生任何 VM-Exit</strong>。因为无论 Guest 怎么改第一级映射，第二级映射（EPT）是由宿主机掌控的，始终安全可靠。</li>
<li><strong>作用</strong>：EPT 极大地减少了内存管理带来的上下文切换次数，虚拟机的内存性能几乎接近物理机。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>VT-x</strong>：解决了<strong>指令执行</strong>的权力和安全问题，变“软件翻译”为“硬件自动拦截”。</li>
<li><strong>EPT</strong>：解决了<strong>内存翻译</strong>的效率问题，变“人工维护影子表”为“硬件二级映射”。</li>
</ul>
<hr>
<h2 id="可以这样理解吗：无论是影子页表还是EPT，都是为了实现虚拟机以为自己有的从0开始的物理地址的幻想，从而定义了一些软件或硬件实现的映射规则？"><a href="#可以这样理解吗：无论是影子页表还是EPT，都是为了实现虚拟机以为自己有的从0开始的物理地址的幻想，从而定义了一些软件或硬件实现的映射规则？" class="headerlink" title="可以这样理解吗：无论是影子页表还是EPT，都是为了实现虚拟机以为自己有的从0开始的物理地址的幻想，从而定义了一些软件或硬件实现的映射规则？"></a>可以这样理解吗：无论是影子页表还是EPT，都是为了实现虚拟机以为自己有的从0开始的物理地址的幻想，从而定义了一些软件或硬件实现的映射规则？</h2><p>你的理解<strong>完全正确</strong>，而且抓住了虚拟化的本质。</p>
<p>虚拟化技术的终极目标就是<strong>“欺骗”</strong>：让虚拟机里的内核觉得自己是这台电脑唯一的、至高无上的主人，拥有从 <code>0</code> 开始的完整内存。</p>
<p>你可以从以下两个维度来巩固这个“幻想”与“映射”的理解：</p>
<h3 id="1-幻想的内容是什么？"><a href="#1-幻想的内容是什么？" class="headerlink" title="1. 幻想的内容是什么？"></a>1. 幻想的内容是什么？</h3><p>在计算机体系结构中，操作系统的“世界观”是基于<strong>地址空间</strong>的。</p>
<ul>
<li><strong>虚拟机内核的幻想</strong>：“我拥有物理地址  到 。我可以随意把任何程序映射到这些地址上。”</li>
<li><strong>残酷的现实</strong>：它以为的  可能是宿主机内存里的 ；它以为连续的内存，在宿主机上可能是碎片化的，甚至被交换到了硬盘上。</li>
</ul>
<h3 id="2-规则的两种实现路径"><a href="#2-规则的两种实现路径" class="headerlink" title="2. 规则的两种实现路径"></a>2. 规则的两种实现路径</h3><p>影子页表和 EPT 就是为了维护这个幻想而制定的两套<strong>“翻译规则”</strong>：</p>
<h4 id="影子页表-Shadow-Page-Tables-——-“软件映射规则”"><a href="#影子页表-Shadow-Page-Tables-——-“软件映射规则”" class="headerlink" title="影子页表 (Shadow Page Tables) —— “软件映射规则”"></a>影子页表 (Shadow Page Tables) —— “软件映射规则”</h4><ul>
<li><strong>原理</strong>：既然硬件（旧款 CPU）只认一张页表，VMM 就把虚拟机的幻想（GVA  GPA）和宿主机的现实（GPA  HPA）<strong>强行压缩</strong>成一张表（GVA  HPA）。</li>
<li><strong>代价</strong>：虚拟机每想改一次自己的幻想（修改页表），都会被 VMM 拦截并重新计算压缩规则。这导致虚拟机“想得越多，卡得越凶”。</li>
</ul>
<h4 id="EPT-Extended-Page-Tables-——-“硬件映射规则”"><a href="#EPT-Extended-Page-Tables-——-“硬件映射规则”" class="headerlink" title="EPT (Extended Page Tables) —— “硬件映射规则”"></a>EPT (Extended Page Tables) —— “硬件映射规则”</h4><ul>
<li><strong>原理</strong>：硬件（新款 CPU）支持了两套并行的规则。</li>
<li><strong>第一套（虚拟机管）</strong>：负责维护“幻想”（GVA  GPA）。</li>
<li><strong>第二套（硬件+VMM管）</strong>：负责把“幻想”落地到“现实”（GPA  HPA）。</li>
</ul>
<ul>
<li><strong>优势</strong>：由于这两套规则在 CPU 内部是<strong>解耦</strong>的，虚拟机可以自由地在自己的幻想世界里折腾页表，而不需要惊动宿主机。硬件会在访问内存的一瞬间，自动完成这两步跳跃。</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>不管是软件模拟还是硬件辅助，它们存在的意义都是<strong>解开“物理地址”这一层硬性耦合</strong>。</p>
<p>如果没有这些规则，虚拟机就像一个试图在别人的领地上盖房子的违章建筑；有了这些规则，虚拟机就像是在一个<strong>平行宇宙</strong>里盖房子，它看到的土地（地址）是它的，但底层支撑的物质（内存）是由宿主机提供的。</p>
<hr>
<h1 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h1><p>我们可以把容器的实现逻辑和本质归纳为一句话：</p>
<p><strong>容器的本质是宿主机上一个（或一组）受到“视觉隔离”和“资源控制”的特殊进程。</strong></p>
<p>以下是其实现的最终总结图景：</p>
<hr>
<h3 id="1-容器实现的三大支柱（技术细节）"><a href="#1-容器实现的三大支柱（技术细节）" class="headerlink" title="1. 容器实现的三大支柱（技术细节）"></a>1. 容器实现的三大支柱（技术细节）</h3><p>容器并不是一个真实的实体，而是由 Linux 内核的三项技术“拼凑”出来的：</p>
<ul>
<li><strong>隔离（Namespaces）—— 虚拟出的“平行宇宙”</strong><br>它通过修改进程对系统资源的视图，实现隔离。每个容器都认为自己拥有独立的 PID 1、独立的网卡 IP 和独立的主机名。<blockquote>
<p><strong>本质：</strong> 内核在处理系统调用时，根据进程所属的 Namespace 动态过滤数据。</p>
</blockquote>
</li>
</ul>
<ul>
<li><strong>限制（Cgroups）—— 强加的“资源配额”</strong><br>它通过树状层级结构，管控进程组对 CPU、内存、磁盘 I/O 等物理资源的使用。<blockquote>
<p><strong>本质：</strong> 内核调度器在分配资源前，先检查该进程所在 Cgroup 的“账本”是否超支。</p>
</blockquote>
</li>
</ul>
<ul>
<li><strong>环境（Rootfs / Mount Namespace）—— 带来的“样板间”</strong><br>通过 <code>pivot_root</code> 指令，将进程的根目录切换到镜像文件所在的目录。<blockquote>
<p><strong>本质：</strong> 进程虽然在宿主机运行，但它看到的 <code>/bin</code>、<code>/etc</code> 全是镜像里的，从而实现环境一致性。</p>
</blockquote>
</li>
</ul>
<hr>
<h3 id="2-容器究竟是什么？（本质定义）"><a href="#2-容器究竟是什么？（本质定义）" class="headerlink" title="2. 容器究竟是什么？（本质定义）"></a>2. 容器究竟是什么？（本质定义）</h3><p>要理解容器，必须跳出“它是一台小电脑”的错觉：</p>
<ol>
<li><strong>它就是进程</strong>：<br>在宿主机执行 <code>ps -ef</code>，你能直接看到容器里的进程。它和 <code>vim</code>、<code>ls</code> 等普通进程在内核调度层面没有区别，都共用同一个内核（Kernel）。</li>
<li><strong>它是 Namespace 的集合</strong>：<br>容器并不对应一个单一的 OSID，而是对应<strong>一组 Namespace 对象的引用</strong>。凡是共享这一组引用的进程，就属于同一个容器。</li>
<li><strong>它是“静态”与“动态”的结合</strong>：</li>
</ol>
<ul>
<li><strong>静态（Image）</strong>：是一堆只读的文件层，提供了程序运行的“土壤”。</li>
<li><strong>动态（Container）</strong>：是基于镜像跑起来的、带了“面具”（Namespace）和“手铐”（Cgroup）的进程。</li>
</ul>
<hr>
<h3 id="3-终极比喻：进程的“楚门的世界”"><a href="#3-终极比喻：进程的“楚门的世界”" class="headerlink" title="3. 终极比喻：进程的“楚门的世界”"></a>3. 终极比喻：进程的“楚门的世界”</h3><ul>
<li><strong>普通进程</strong>：生活在真实的城市里，能看到街道上的所有人，能去任何想去的地方（受限于基本权限）。</li>
<li><strong>容器进程</strong>：就像<strong>楚门（Truman）</strong>。导演（VMM/Docker）为他建造了一个巨大的摄影棚：</li>
<li><strong>Namespace</strong> 是布景墙：让他以为这个小镇就是全世界，以为自己是这里的 1 号人物。</li>
<li><strong>Cgroups</strong> 是剧组预算：规定他每天只能吃两顿饭，不能跑太快。</li>
<li><strong>Rootfs</strong> 是道具：家里所有的家具（文件）都是剧组提前放好的样板。</li>
<li><strong>本质</strong>：楚门依然是一个活生生的人（进程），但他生活在一个被严格限制的、虚假的隔离环境里。</li>
</ul>
<hr>
<h3 id="4-总结对比：VM-vs-Container"><a href="#4-总结对比：VM-vs-Container" class="headerlink" title="4. 总结对比：VM vs Container"></a>4. 总结对比：VM vs Container</h3><div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>虚拟机 (VM)</th>
<th>容器 (Container)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>本质</strong></td>
<td><strong>硬件级虚拟化</strong></td>
<td><strong>操作系统级虚拟化</strong></td>
</tr>
<tr>
<td><strong>隔离边界</strong></td>
<td>整个硬件（通过 Hypervisor）</td>
<td>进程（通过内核 Namespaces）</td>
</tr>
<tr>
<td><strong>启动速度</strong></td>
<td>分钟级（需启动 Guest OS）</td>
<td>毫秒级（直接启动进程）</td>
</tr>
<tr>
<td><strong>资源损耗</strong></td>
<td>高（需运行完整内核 + EPT 映射）</td>
<td>极低（直接共享宿主机内核）</td>
</tr>
<tr>
<td><strong>安全性</strong></td>
<td>极强（硬件屏障）</td>
<td>较弱（共享内核，存在逃逸风险）</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="所以容器究竟对应的是进程还是进程组呢"><a href="#所以容器究竟对应的是进程还是进程组呢" class="headerlink" title="所以容器究竟对应的是进程还是进程组呢?"></a>所以容器究竟对应的是进程还是进程组呢?</h3><p>这是一个非常敏锐的问题。简单的回答是：<strong>容器在 Linux 内核眼中是一个或一组进程，但在管理层面，它通常被视为一个“进程组”。</strong></p>
<p>为了彻底理清这个概念，我们需要从<strong>“初始进程”</strong>和<strong>“Namespace 共享”</strong>两个角度来看：</p>
<h4 id="1-容器的起点：PID-1-进程"><a href="#1-容器的起点：PID-1-进程" class="headerlink" title="1. 容器的起点：PID 1 进程"></a>1. 容器的起点：PID 1 进程</h4><p>当你启动一个容器（例如 <code>docker run nginx</code>）时，内核实际上只启动了<strong>一个</strong>进程。</p>
<ul>
<li>这个进程就是容器的<strong>初始进程（Entrypoint）</strong>。</li>
<li>在该容器的 PID Namespace 内部，这个进程的 PID 是 <strong>1</strong>。</li>
<li>这个 PID 1 进程是整个容器的“根”。如果这个进程退出了，内核会认为容器已经停止，从而回收所有的 Namespace 和 Cgroup 资源。</li>
</ul>
<h4 id="2-容器的扩展：进程的集合"><a href="#2-容器的扩展：进程的集合" class="headerlink" title="2. 容器的扩展：进程的集合"></a>2. 容器的扩展：进程的集合</h4><p>虽然容器启动时只有一个进程，但这个进程可以产生子进程。</p>
<ul>
<li>比如，你在 Nginx 容器里执行 <code>ls</code> 命令，或者 Nginx 启动了多个 Worker 进程。</li>
<li><strong>这些子进程会继承父进程的 Namespace 和 Cgroup 限制。</strong></li>
<li>此时，这个容器就变成了一个<strong>进程组</strong>（这些进程共享同一套隔离环境和资源配额）。</li>
</ul>
<h4 id="3-核心结论：用-Namespace-来定义边界"><a href="#3-核心结论：用-Namespace-来定义边界" class="headerlink" title="3. 核心结论：用 Namespace 来定义边界"></a>3. 核心结论：用 Namespace 来定义边界</h4><p>在 Linux 底层，并没有“容器”这个专门的数据结构。<strong>容器其实是“共享同一组 Namespace 的所有进程的集合”。</strong></p>
<p>我们可以这样拆解：</p>
<ul>
<li><strong>从内核数据结构看</strong>：它是多个 <code>task_struct</code>（进程结构体），它们的指针都指向了同一组 <code>nsproxy</code>（Namespace 代理对象）。</li>
<li><strong>从资源控制（Cgroup）看</strong>：它是一个 <strong>Control Group</strong>，里面记录了一连串的 PID。无论这个容器里运行了 1 个还是 100 个进程，它们产生的功耗都会累加在这个 Cgroup 账本上。</li>
</ul>
<hr>
<h3 id="既然容器只是进程，为什么它能在-Windows-或-Mac-上运行？（提示：它们其实在偷偷跑一个极小的-Linux-内核）"><a href="#既然容器只是进程，为什么它能在-Windows-或-Mac-上运行？（提示：它们其实在偷偷跑一个极小的-Linux-内核）" class="headerlink" title="既然容器只是进程，为什么它能在 Windows 或 Mac 上运行？（提示：它们其实在偷偷跑一个极小的 Linux 内核）"></a>既然容器只是进程，为什么它能在 Windows 或 Mac 上运行？（提示：它们其实在偷偷跑一个极小的 Linux 内核）</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/21/ML%E5%88%9D%E6%AD%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/21/ML%E5%88%9D%E6%AD%A5/" class="post-title-link" itemprop="url">机器学习理论基石：从PAC学习框架到偏差方差权衡</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-21 23:00:00 / Modified: 23:33:32" itemprop="dateCreated datePublished" datetime="2025-12-21T23:00:00+08:00">2025-12-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E6%80%9D%E8%80%83/" itemprop="url" rel="index"><span itemprop="name">深度思考</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>为什么机器学习能奏效？本文深入浅出地探讨PAC学习框架、无限假设空间下的VC维，以及经典的偏差-方差分解，带你理解算法背后的理论边界。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/12/21/ML%E5%88%9D%E6%AD%A5/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/12/21/import/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/21/import/" class="post-title-link" itemprop="url">硬核解析：Python 模块化编程与 Import 机制的底层逻辑</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-21 16:30:00 / Modified: 15:41:50" itemprop="dateCreated datePublished" datetime="2025-12-21T16:30:00+08:00">2025-12-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python%E8%BF%9B%E9%98%B6/" itemprop="url" rel="index"><span itemprop="name">Python进阶</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python%E8%BF%9B%E9%98%B6/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/" itemprop="url" rel="index"><span itemprop="name">架构设计</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>“为什么 <code>import</code> 总是报错 <code>ModuleNotFoundError</code>？”<br>“为什么我的代码里会出现 <code>ImportError: cannot import name</code> 的循环依赖？”<br>“<code>__init__.py</code> 到底需不需要写？”</p>
<p>对于初级开发者，<code>import</code> 只是一个语法；但对于进阶工程师，理解 Python 的模块系统（Module System）是构建大型、可维护项目的基石。本文将从底层原理出发，拆解 <code>import</code> 的执行流程，并给出工程化的最佳实践。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/12/21/import/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
